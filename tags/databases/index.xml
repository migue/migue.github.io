<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>databases | Just my thouhgts</title>
    <link>https://migue.github.io/tags/databases/</link>
      <atom:link href="https://migue.github.io/tags/databases/index.xml" rel="self" type="application/rss+xml" />
    <description>databases</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 14 Apr 2020 17:14:30 +0100</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>databases</title>
      <link>https://migue.github.io/tags/databases/</link>
    </image>
    
    <item>
      <title>InfluxDB storage subsystem: the TSM files</title>
      <link>https://migue.github.io/post/influx-storage-tsm-component/</link>
      <pubDate>Tue, 14 Apr 2020 17:14:30 +0100</pubDate>
      <guid>https://migue.github.io/post/influx-storage-tsm-component/</guid>
      <description>&lt;p&gt;During this entry, we are going through the TSM part of the InfluxDB storage engine: how the contents are organized in the disk, how they are compressed or how they are compacted. This is the second entry of the series about the InfluxDB storage engine started in 
&lt;a href=&#34;https://migue.github.io/post/quick-tour-influx-storage/&#34;&gt;the previous post&lt;/a&gt;
.&lt;/p&gt;
&lt;h1 id=&#34;tsm-files-structure&#34;&gt;TSM files structure&lt;/h1&gt;
&lt;p&gt;The TSM files are where Influx stores the real data; these files are read-only files and are memory-mapped. If you’re familiar with any database using an LSM Tree variant this concept is very similar to the SSTable concept.&lt;/p&gt;
&lt;p&gt;Let’s start with the structure of the files and how they are physically stored. At a high level, the structure is shown in the picture below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/influxdb-tsm-file-structure.png&#34; alt=&#34;TSM File Structure&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-header&#34;&gt;The header&lt;/h2&gt;
&lt;p&gt;The header is a magic number which helps to identify the type of the file (4 bytes) and its version number (1 byte):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/influx-tsm-file-structure-header.png&#34; alt=&#34;TSM File Structure Header Section&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-blocks&#34;&gt;The blocks&lt;/h2&gt;
&lt;p&gt;Blocks are sequences of pairs where every pair is composed of a CRC32 checksum and the data that needs to be stored. The diagram below shows how this part is structured:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/influx-tsm-structure-data-blocks.png&#34; alt=&#34;TSM File Structure Blocks Section&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-index&#34;&gt;The index&lt;/h2&gt;
&lt;p&gt;The index section serves, as its suggest, as the index to the set of blocks in the file and is composed of a sequence of entries lexicographically ordered by key first and then by time. The format of every entry in the previous sequence is shown in the next diagram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/influx-tsm-structure-index.png&#34; alt=&#34;TSM File Structure Index Section&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KeyLen&lt;/strong&gt;: represents the length of the key.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key&lt;/strong&gt;: represents the key itself.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: represents the type of the field being stored (float, integer, string or bool).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Count&lt;/strong&gt;: represents the number of blocks in the file.
For every block in the TSM file, there is a corresponding entry in the index with the following information:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MinTime&lt;/strong&gt;: minimum time stored in the block&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MaxTime&lt;/strong&gt;: maximum time stored in the block&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offset&lt;/strong&gt;: the offset into the file where the block is located&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt;: the size of the block&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note how this index allows the database to efficiently access all the required blocks. When a key and a date are provided the database knows exactly which file contains the block and where this block is located and how much data needs to be read to retrieve the aforementioned block.&lt;/p&gt;
&lt;h2 id=&#34;the-footer&#34;&gt;The footer&lt;/h2&gt;
&lt;p&gt;The last section of the TMS file is the footer that stores the offset where the index starts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/influx-tsm-structure-footer.png&#34; alt=&#34;TSM File Structure Footer Section&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we have already mentioned in the previous post, when the cache is full a snapshot is written to the corresponding TSM file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// WriteSnapshot will snapshot the cache and write a new TSM file with its contents, releasing the snapshot when done.
func (e *Engine) WriteSnapshot() (err error) {
    ...
    
    return e.writeSnapshotAndCommit(log, closedFiles, snapshot)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous snippet just highlights where the actual writing process is invoked; you can find more details 
&lt;a href=&#34;https://github.com/influxdata/influxdb/blob/1.8/tsdb/engine/tsm1/engine.go#L1903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
.&lt;/p&gt;
&lt;h1 id=&#34;tsm-file-compression&#34;&gt;TSM File compression&lt;/h1&gt;
&lt;p&gt;Every data block is actually compressed before being persisted into the disk in order to reduce both IO operations and disk space. The structure of a block is shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/influx-tsm-data-block-structure.png&#34; alt=&#34;TSM Data Block Structure&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you look carefully at the previous picture you can see that timestamps and the actual values are encoded separately, allowing the engine to use timestamp encoding for all the timestamps and the more appropriate encoding for every one of the fields. I think this has been a great decision and the usual compression ratios seem to validate this decision (I’ve seen compression ratios of 1:23, 1:24 in a few differente scenarios).&lt;/p&gt;
&lt;p&gt;Complementing the timestamps and values encodings every block starts with a 1-byte-header where the four higher bits define the compression type and the four lower bits are there for the encoder in case it needs them. Right after this header, using a variable byte encoding mechanism, the length of the timestamps block is stored.&lt;/p&gt;
&lt;p&gt;The compression mechanisms for every type of data are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Timestamps&lt;/strong&gt;: an adaptive approach based on the structure of the timestamps to be encoded is used. It’s a combination of delta encoding, scaling and compression using 
&lt;a href=&#34;https://github.com/jwilder/encoding/blob/master/simple8b/encoding.go&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple8b&lt;/a&gt;
 run-length encoding (falling back to no compression in case it’s needed). You can find more details about this approach in the 
&lt;a href=&#34;http://www.vldb.org/pvldb/vol8/p1816-teller.pdf?spm=a2c65.11461447.0.0.4a976b213iTmnM&amp;amp;file=p1816-teller.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gorrilla paper&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Floats&lt;/strong&gt;: I think, again, this encoding is based in the aforementioned Gorilla paper. If you’re interested to learn more about it, the paper has a nice explanation about its inner workings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integers&lt;/strong&gt;: Two different strategies are used to compress integers depending on the range values of the data that needs to be compressed. As a first step, the values are encoded using [Zig Zag encoding] (&lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/encoding#signed-integers)&#34;&gt;https://developers.google.com/protocol-buffers/docs/encoding#signed-integers)&lt;/a&gt;. If the value is smaller than &lt;code&gt;(1 &amp;lt;&amp;lt; 60) - 1&lt;/code&gt; they are compressed the simple8b algorithm mentioned above and, if they are bigger, they are stored uncompressed. You can see where this decision is made in the next code snippet (extracted from 
&lt;a href=&#34;https://github.com/influxdata/influxdb/blob/1.8/tsdb/engine/tsm1/batch_integer.go#L74&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
):&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;if max &amp;gt; simple8b.MaxValue { // There is an encoded value that&#39;s too big to simple8b encode.
        // Encode uncompressed.
        sz := 1 + len(deltas)*8
        if len(b) &amp;lt; sz &amp;amp;&amp;amp; cap(b) &amp;gt;= sz {
            b = b[:sz]
        } else if len(b) &amp;lt; sz {
            b = append(b, make([]byte, sz-len(b))...)
        }
 
        // 4 high bits of first byte store the encoding type for the block
        b[0] = byte(intUncompressed) &amp;lt;&amp;lt; 4
        for i, v := range deltas {
            binary.BigEndian.PutUint64(b[1+i*8:1+i*8+8], uint64(v))
        }
        return b[:sz], nil
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Booleans&lt;/strong&gt;: they are encoded using a bit packing strategy (each boolean use 1 bit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Strings&lt;/strong&gt;: they are encoded using 
&lt;a href=&#34;http://google.github.io/snappy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Snappy&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;compaction-process&#34;&gt;Compaction process&lt;/h1&gt;
&lt;p&gt;So far we’ve seen how all our points are physically stored in the disk and the reasoning behind the decision to use such data layout.
Aiming to optimize the storage of the previous data from the query perspective, Influx continuously runs a compaction process. There’re a few different levels of compaction types&lt;/p&gt;
&lt;h2 id=&#34;snapshot&#34;&gt;Snapshot&lt;/h2&gt;
&lt;p&gt;We’ve already talked briefly about this process; The values stored in the Cache and the WAL need to be stored in a TSM file in order to save both memory and disk space. If you remember for the previous post, we’ve already described this process in the previous post.&lt;/p&gt;
&lt;h2 id=&#34;leveled-compactions&#34;&gt;Leveled compactions&lt;/h2&gt;
&lt;p&gt;There are 4 different levels of compaction and they occur as the size of the TSM grows. Snapshots are compacted to level-1 files, level-1 files are compacted to level-2 files and so on. When level-4 is reached no further compaction is applied to these files.&lt;/p&gt;
&lt;p&gt;Going deeper into the inner workings of the leveled compaction process would take a whole separate blog entry so I am going to stop here&lt;/p&gt;
&lt;h2 id=&#34;index-optimization&#34;&gt;Index optimization&lt;/h2&gt;
&lt;p&gt;In the scenario where many level-4 TSM files are created, the index becomes larger and the cost of accessing it increases. This optimization tries to split the series across different TSM files, sorting all points for a particular series into the same file.&lt;/p&gt;
&lt;p&gt;Before this process, a TSM file stores points about most of the series and, after the optimization is executed, each of the new TSM files store a reduced number of series (with very little overlap among them).&lt;/p&gt;
&lt;h2 id=&#34;full-compactions&#34;&gt;Full compactions&lt;/h2&gt;
&lt;p&gt;This type of compaction process only runs when a shard has become cold for writes (no new writes are coming into it) or when a delete operation is executed on the shard. This compaction process applies all the techniques used in the leveled compactions and the index optimization process&lt;/p&gt;
&lt;h1 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;In this post, I’ve covered a few details about the TSM part of the InfluxDB’s storage engine, going a little bit deeper into some of the concepts introduced in the first post of this series.&lt;/p&gt;
&lt;p&gt;In the next post, I will try to provide a few details about the TSI files and how this part of the storage subsystem helps Influx to speed up more complex queries.&lt;/p&gt;
&lt;p&gt;Again, we’ve used InfluxDB as the vehicle to show some of the concepts used for building a database: how the information is organized in the disk, compression, efficiently looking for information, … Of course, this will be dependent on the type of database being built.&lt;/p&gt;
&lt;p&gt;Thanks a lot for reading! I hope you have enjoyed it as much as I have done writing it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>InfluxDB storage subsystem: an introduction</title>
      <link>https://migue.github.io/post/quick-tour-influx-storage/</link>
      <pubDate>Tue, 14 Apr 2020 15:14:30 +0100</pubDate>
      <guid>https://migue.github.io/post/quick-tour-influx-storage/</guid>
      <description>&lt;p&gt;Writing on the blog about some of the technical stuff I usually enjoy and/or work with is something that’s always in my plans but I never find the right time do it. Due to all this quarantine related stuff, both my sleeping habits and the kid’s are being somehow affected and, well, here I am, staring at a blank page trying to start writing about databases. And, well, since I don’t longer work for an international company I thought it would be a good idea to somehow practice my poor English skills.&lt;/p&gt;
&lt;p&gt;This time my plan is to write about InfluxDB, a columnar oriented time-series database written in Go, and provide a quick tour of some of the most important characteristics of their storage engine.&lt;/p&gt;
&lt;p&gt;I have been dealing with InfluxDB for a while and I have gone way deeper than I would like to quite a few times but &lt;strong&gt;I am by no means an expert&lt;/strong&gt;, so, please, if I say something that’s not completely accurate, forgive me, and please, correct me.&lt;/p&gt;
&lt;p&gt;I’ve dealt with both the OpenSource and the Enterprise versions but everything here is going to be based on the former (to the best of my knowledge the storage engine is the same in both alternatives). The details included later in the post are based on the 1.8.x and 1.7.x branches (I know the 2.0 introduces quite a lot changes but I won’t talk about them here).&lt;/p&gt;
&lt;h1 id=&#34;before-getting-started&#34;&gt;Before getting started&lt;/h1&gt;
&lt;p&gt;Influx has gone through different storage engines through its short lifetime: LevelDB, BoltDB (not sure if there’s anymore) and, and this is just an educated guess, any of them completely satisfied the requirements that the InfluxDB folks were looking for: large batch deletes, hot backups, high throughput or a good compression performance among many others. So, in order to solve the previous points (it’s not an exhaustive list), they decided to go on their own and write a new storage engine (it&amp;rsquo;s a brave bet).&lt;/p&gt;
&lt;p&gt;Everything I am going to write about here is focused on this storage engine and I have no experience with the Influx versions where the underlying storage engine was LevelDB or BoltDB.&lt;/p&gt;
&lt;h1 id=&#34;influxdb-concepts&#34;&gt;InfluxDB concepts&lt;/h1&gt;
&lt;p&gt;Before we go into the details of the storage engine let me introduce a few concepts I think all of us should know about.&lt;/p&gt;
&lt;p&gt;Everything starts with a database concept, similar to a traditional RDBMS, which acts as the container of many of the capabilities that Influx provides: user management, retention policies, continuous queries, … Everything related to a database is represented under a folder in the filesystem.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;retention policy&lt;/strong&gt; describes how long the data is kept around and how many copies of this data are stored in the cluster (for the OpenSource version the replication setting has no effect).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Measurements&lt;/strong&gt; are the “data structure” used to model how the data is stored in Influx and the fields associated with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fields&lt;/strong&gt; represent the actual data value. These are required and, something really important, they are not indexed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tags&lt;/strong&gt; represents metadata associated with the aforementioned fields. They are optional, but they are very useful because they are indexed and allow you to perform group by and/or filter operations (you can filter on fields as well, but since they are not indexed, this is not a performant operation).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Series&lt;/strong&gt; are a logical grouping of data defined by a measurement, a set of tags and a field. To me, this is one of the most important concepts that need to be clear while working with InfluxDB, because many of the different concepts revolve around it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shards&lt;/strong&gt;  contains the actual encoded and compressed data. They are represented by a TSM file on disk (more on this later). Every shard contains a specific set of series so all the points falling on a given series will be stored in the same shard.&lt;/p&gt;
&lt;p&gt;In order to get your data points inserted into the database, Influx defines a text-based protocol named &lt;strong&gt;line protocol&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;An example of how a single data point looks like in the &lt;strong&gt;line protocol&lt;/strong&gt; and how it maps to the concepts described before is shown in the picture below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/influx-line-protocol.png&#34; alt=&#34;Influx Line Protocol&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;influxdb-storage-engine&#34;&gt;InfluxDB storage engine&lt;/h1&gt;
&lt;p&gt;At the beginning of the post, I described some of the features that, per my understanding, the Influx folks were looking for when building their storage engine and how all these requirements led them to their current storage solution.&lt;/p&gt;
&lt;p&gt;Their storage solution is similar to an LSM Tree and, from a high-level perspective, it is composed by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A write-ahead log&lt;/li&gt;
&lt;li&gt;A collection of &lt;strong&gt;TSM&lt;/strong&gt; (read-only) files where the actual time series data is stored (similar to the SSTables)&lt;/li&gt;
&lt;li&gt;TSI files that serve as the inverted index used to quickly access the information. Prior to this version of the index, the data was held into an in-memory data structure, making it difficult to support high-cardinality scenarios (millions of series).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before we move forward, a short note about how I plan to organize the contents. The original idea was to put everything I wanted to write down into a single post but it would be probably a little bit dense so I have decided to split it into 3 different parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The current one with the introduction to Influx, the overview of the storage engine and a quick tour around the WAL component and its cache counterpart.&lt;/li&gt;
&lt;li&gt;The second one will cover the details about the TSM.&lt;/li&gt;
&lt;li&gt;The third one will cover details about the TSI.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this post, let’s try to go a little bit deeper into the write ahead log (WAL) and its cache counterpart.&lt;/p&gt;
&lt;h2 id=&#34;write-ahead-log-wal&#34;&gt;Write ahead log (WAL)&lt;/h2&gt;
&lt;p&gt;The WAL is a write-optimized data structure that allows the writes to be durable and its main goal is to allow the writes to be appended as fast as possible so it is not easily queryable.&lt;/p&gt;
&lt;p&gt;When a new write comes into the system the new points are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stored at an in-memory cache (more about this in the next section).&lt;/li&gt;
&lt;li&gt;Serialized, compressed using Snappy and written to the WAL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we take a look at the 
&lt;a href=&#34;https://github.com/influxdata/influxdb/blob/1.8/tsdb/engine/tsm1/engine.go#L1367&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source code&lt;/a&gt;
 we can see where this actually happens:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// WritePoints writes metadata and point data into the engine.
// It returns an error if new points are added to an existing key.
func (e *Engine) WritePoints(points []models.Point) error {
    ….
    // first try to write to the cache
    if err := e.Cache.WriteMulti(values); err != nil {
        return err
    }
 
    if e.WALEnabled {
        if _, err := e.WAL.WriteMulti(values); err != nil {
            return err
        }
    }
    return seriesErr
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we could disable the WAL and, in this case, writes will only exist in the cache and could be lost if a cache snapshot has not happened.&lt;/p&gt;
&lt;p&gt;The format used to describe every one of the entries appended to the WAL follows a type-length-value encoding scheme with a single byte representing the type of the entry being stored (write, delete or a range delete), a 4 byte uint32 representing the length of the compressed block, followed by the actual compressed data block.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/wal-entry-encoding.png&#34; alt=&#34;Wal entry encoding&#34;&gt;&lt;/p&gt;
&lt;p&gt;Looking again at the 
&lt;a href=&#34;https://github.com/influxdata/influxdb/blob/1.8/tsdb/engine/tsm1/wal.go#L1062&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source code&lt;/a&gt;
 here we can see how the actual writing is performed:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Write writes entryType and the buffer containing compressed entry data.
func (w *WALSegmentWriter) Write(entryType WalEntryType, compressed []byte) error {
    var buf [5]byte
    buf[0] = byte(entryType)
    binary.BigEndian.PutUint32(buf[1:5], uint32(len(compressed)))
 
    if _, err := w.bw.Write(buf[:]); err != nil {
        return err
    }
 
    if _, err := w.bw.Write(compressed); err != nil {
        return err
    }
 
    w.size += len(buf) + len(compressed)
 
    return nil
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cache&#34;&gt;Cache&lt;/h2&gt;
&lt;p&gt;The cache component is an in-memory data structure that holds a copy of all the points persisted in the WAL. As we’ve already mentioned in the previous section, when a new write comes into the system the new points are stored in this cache.&lt;/p&gt;
&lt;p&gt;The aforementioned points are indexed by the key which is formed by the measurement name, the tag set and the unique field key. If we go back to our previous example where we had a single write coming into the system:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;infrastructure_metrics,server=server-1,container=container-1 cpu_usage=82
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key would be represented by something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://migue.github.io/img/wal-cache-key.png&#34; alt=&#34;WAL Cache Key&#34;&gt;&lt;/p&gt;
&lt;p&gt;The points stored in the cache are not compressed and an upper bound can be set so we can prevent unexpected out of memory situations (if the upper bound limit is exceeded new writes coming into the system will be rejected) and prevent the database service to be unexpectedly restarted.&lt;/p&gt;
&lt;p&gt;When the number of elements in the cache reaches a certain lower bound (it’s configurable as well) a snapshot of the cache is triggered to a TSM file and the corresponding WAL segment files are removed.&lt;/p&gt;
&lt;p&gt;If we take a quick look to the 
&lt;a href=&#34;https://github.com/influxdata/influxdb/blob/1.8/tsdb/engine/tsm1/engine.go#L1968&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source code&lt;/a&gt;
 we can see the behavior described in the previous paragraph:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// compactCache continually checks if the WAL cache should be written to disk.
func (e *Engine) compactCache() {
    t := time.NewTicker(time.Second)
    defer t.Stop()
    for {
        e.mu.RLock()
        quit := e.snapDone
        e.mu.RUnlock()
 
        select {
        case &amp;lt;-quit:
            return
 
        case &amp;lt;-t.C:
            e.Cache.UpdateAge()
            if e.ShouldCompactCache(time.Now()) {
                start := time.Now()
                e.traceLogger.Info(&amp;quot;Compacting cache&amp;quot;, zap.String(&amp;quot;path&amp;quot;, e.path))
                err := e.WriteSnapshot()
                if err != nil &amp;amp;&amp;amp; err != errCompactionsDisabled {
                    e.logger.Info(&amp;quot;Error writing snapshot&amp;quot;, zap.Error(err))
                    atomic.AddInt64(&amp;amp;e.stats.CacheCompactionErrors, 1)
                } else {
                    atomic.AddInt64(&amp;amp;e.stats.CacheCompactions, 1)
                }
                atomic.AddInt64(&amp;amp;e.stats.CacheCompactionDuration, time.Since(start).Nanoseconds())
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When a new read operation is received, the storage engine will merge data from the TSM files with the data stored in the cache (so you can read data that hasn’t been snapshotted into the TSM files yet). At query processing time, a copy of the data is done so any new write coming into the system won’t affect the results of any running query.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This post has been a short intro to InfluxDB and a brief overview of its storage subsystem. In addition to the previous intros, we’ve covered a few details of the WAL + Cache storage subsystem elements. As I have already mentioned, my idea is to publish two more posts: one of them covering a few details of the TSM part and the other one going through the inner workings of the TSI component.&lt;/p&gt;
&lt;p&gt;Of course, I am aware this is just an introduction, and the devil is in the details, but I do hope this provides you some insights into a few database design concepts and how a concrete database applies them.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
