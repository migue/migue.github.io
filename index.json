[{"authors":["admin"],"categories":null,"content":"I am a proud dad and husband, software architect, speaker, and writer. Passionate reader, chef aficionado, former surf player and current cyclist and runner.\nI am unsuccessfully pursuing to move my Phd research forward.\n","date":1586880870,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586880870,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://migue.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a proud dad and husband, software architect, speaker, and writer. Passionate reader, chef aficionado, former surf player and current cyclist and runner.\nI am unsuccessfully pursuing to move my Phd research forward.","tags":null,"title":"Miguel Ángel Pastor Olivar","type":"authors"},{"authors":["Miguel Ángel Pastor Olivar"],"categories":["Computer Science","Programming"],"content":"During this entry, we are going through the TSM part of the InfluxDB storage engine: how the contents are organized in the disk, how they are compressed or how they are compacted. This is the second entry of the series about the InfluxDB storage engine started in the previous post .\nTSM files structure The TSM files are where Influx stores the real data; these files are read-only files and are memory-mapped. If you’re familiar with any database using an LSM Tree variant this concept is very similar to the SSTable concept.\nLet’s start with the structure of the files and how they are physically stored. At a high level, the structure is shown in the picture below:\nThe header The header is a magic number which helps to identify the type of the file (4 bytes) and its version number (1 byte):\nThe blocks Blocks are sequences of pairs where every pair is composed of a CRC32 checksum and the data that needs to be stored. The diagram below shows how this part is structured:\nThe index The index section serves, as its suggest, as the index to the set of blocks in the file and is composed of a sequence of entries lexicographically ordered by key first and then by time. The format of every entry in the previous sequence is shown in the next diagram:\n KeyLen: represents the length of the key. Key: represents the key itself. Type: represents the type of the field being stored (float, integer, string or bool). Count: represents the number of blocks in the file. For every block in the TSM file, there is a corresponding entry in the index with the following information: MinTime: minimum time stored in the block MaxTime: maximum time stored in the block Offset: the offset into the file where the block is located Size: the size of the block  Note how this index allows the database to efficiently access all the required blocks. When a key and a date are provided the database knows exactly which file contains the block and where this block is located and how much data needs to be read to retrieve the aforementioned block.\nThe footer The last section of the TMS file is the footer that stores the offset where the index starts.\nAs we have already mentioned in the previous post, when the cache is full a snapshot is written to the corresponding TSM file:\n// WriteSnapshot will snapshot the cache and write a new TSM file with its contents, releasing the snapshot when done. func (e *Engine) WriteSnapshot() (err error) { ... return e.writeSnapshotAndCommit(log, closedFiles, snapshot) }  The previous snippet just highlights where the actual writing process is invoked; you can find more details here .\nTSM File compression Every data block is actually compressed before being persisted into the disk in order to reduce both IO operations and disk space. The structure of a block is shown below:\nIf you look carefully at the previous picture you can see that timestamps and the actual values are encoded separately, allowing the engine to use timestamp encoding for all the timestamps and the more appropriate encoding for every one of the fields. I think this has been a great decision and the usual compression ratios seem to validate this decision (I’ve seen compression ratios of 1:23, 1:24 in a few differente scenarios).\nComplementing the timestamps and values encodings every block starts with a 1-byte-header where the four higher bits define the compression type and the four lower bits are there for the encoder in case it needs them. Right after this header, using a variable byte encoding mechanism, the length of the timestamps block is stored.\nThe compression mechanisms for every type of data are:\n  Timestamps: an adaptive approach based on the structure of the timestamps to be encoded is used. It’s a combination of delta encoding, scaling and compression using Simple8b run-length encoding (falling back to no compression in case it’s needed). You can find more details about this approach in the Gorrilla paper   Floats: I think, again, this encoding is based in the aforementioned Gorilla paper. If you’re interested to learn more about it, the paper has a nice explanation about its inner workings.\n  Integers: Two different strategies are used to compress integers depending on the range values of the data that needs to be compressed. As a first step, the values are encoded using [Zig Zag encoding] (https://developers.google.com/protocol-buffers/docs/encoding#signed-integers). If the value is smaller than (1 \u0026lt;\u0026lt; 60) - 1 they are compressed the simple8b algorithm mentioned above and, if they are bigger, they are stored uncompressed. You can see where this decision is made in the next code snippet (extracted from here ):\n  if max \u0026gt; simple8b.MaxValue { // There is an encoded value that's too big to simple8b encode. // Encode uncompressed. sz := 1 + len(deltas)*8 if len(b) \u0026lt; sz \u0026amp;\u0026amp; cap(b) \u0026gt;= sz { b = b[:sz] } else if len(b) \u0026lt; sz { b = append(b, make([]byte, sz-len(b))...) } // 4 high bits of first byte store the encoding type for the block b[0] = byte(intUncompressed) \u0026lt;\u0026lt; 4 for i, v := range deltas { binary.BigEndian.PutUint64(b[1+i*8:1+i*8+8], uint64(v)) } return b[:sz], nil }   Booleans: they are encoded using a bit packing strategy (each boolean use 1 bit) Strings: they are encoded using Snappy   Compaction process So far we’ve seen how all our points are physically stored in the disk and the reasoning behind the decision to use such data layout. Aiming to optimize the storage of the previous data from the query perspective, Influx continuously runs a compaction process. There’re a few different levels of compaction types\nSnapshot We’ve already talked briefly about this process; The values stored in the Cache and the WAL need to be stored in a TSM file in order to save both memory and disk space. If you remember for the previous post, we’ve already described this process in the previous post.\nLeveled compactions There are 4 different levels of compaction and they occur as the size of the TSM grows. Snapshots are compacted to level-1 files, level-1 files are compacted to level-2 files and so on. When level-4 is reached no further compaction is applied to these files.\nGoing deeper into the inner workings of the leveled compaction process would take a whole separate blog entry so I am going to stop here\nIndex optimization In the scenario where many level-4 TSM files are created, the index becomes larger and the cost of accessing it increases. This optimization tries to split the series across different TSM files, sorting all points for a particular series into the same file.\nBefore this process, a TSM file stores points about most of the series and, after the optimization is executed, each of the new TSM files store a reduced number of series (with very little overlap among them).\nFull compactions This type of compaction process only runs when a shard has become cold for writes (no new writes are coming into it) or when a delete operation is executed on the shard. This compaction process applies all the techniques used in the leveled compactions and the index optimization process\nWrapping up In this post, I’ve covered a few details about the TSM part of the InfluxDB’s storage engine, going a little bit deeper into some of the concepts introduced in the first post of this series.\nIn the next post, I will try to provide a few details about the TSI files and how this part of the storage subsystem helps Influx to speed up more complex queries.\nAgain, we’ve used InfluxDB as the vehicle to show some of the concepts used for building a database: how the information is organized in the disk, compression, efficiently looking for information, … Of course, this will be dependent on the type of database being built.\nThanks a lot for reading! I hope you have enjoyed it as much as I have done writing it.\n","date":1586880870,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586880870,"objectID":"2ff5b017aa12d14632865b9984c15cdd","permalink":"https://migue.github.io/post/influx-storage-tsm-component/","publishdate":"2020-04-14T17:14:30+01:00","relpermalink":"/post/influx-storage-tsm-component/","section":"post","summary":"During this entry, we are going through the TSM part of the InfluxDB storage engine: how the contents are organized in the disk, how they are compressed or how they are compacted. This is the second entry of the series about the InfluxDB storage engine started in the previous post .\nTSM files structure The TSM files are where Influx stores the real data; these files are read-only files and are memory-mapped.","tags":["databases"],"title":"InfluxDB storage subsystem: the TSM files","type":"post"},{"authors":["Miguel Ángel Pastor Olivar"],"categories":["Computer Science","Programming"],"content":"Writing on the blog about some of the technical stuff I usually enjoy and/or work with is something that’s always in my plans but I never find the right time do it. Due to all this quarantine related stuff, both my sleeping habits and the kid’s are being somehow affected and, well, here I am, staring at a blank page trying to start writing about databases. And, well, since I don’t longer work for an international company I thought it would be a good idea to somehow practice my poor English skills.\nThis time my plan is to write about InfluxDB, a columnar oriented time-series database written in Go, and provide a quick tour of some of the most important characteristics of their storage engine.\nI have been dealing with InfluxDB for a while and I have gone way deeper than I would like to quite a few times but I am by no means an expert, so, please, if I say something that’s not completely accurate, forgive me, and please, correct me.\nI’ve dealt with both the OpenSource and the Enterprise versions but everything here is going to be based on the former (to the best of my knowledge the storage engine is the same in both alternatives). The details included later in the post are based on the 1.8.x and 1.7.x branches (I know the 2.0 introduces quite a lot changes but I won’t talk about them here).\nBefore getting started Influx has gone through different storage engines through its short lifetime: LevelDB, BoltDB (not sure if there’s anymore) and, and this is just an educated guess, any of them completely satisfied the requirements that the InfluxDB folks were looking for: large batch deletes, hot backups, high throughput or a good compression performance among many others. So, in order to solve the previous points (it’s not an exhaustive list), they decided to go on their own and write a new storage engine (it\u0026rsquo;s a brave bet).\nEverything I am going to write about here is focused on this storage engine and I have no experience with the Influx versions where the underlying storage engine was LevelDB or BoltDB.\nInfluxDB concepts Before we go into the details of the storage engine let me introduce a few concepts I think all of us should know about.\nEverything starts with a database concept, similar to a traditional RDBMS, which acts as the container of many of the capabilities that Influx provides: user management, retention policies, continuous queries, … Everything related to a database is represented under a folder in the filesystem.\nA retention policy describes how long the data is kept around and how many copies of this data are stored in the cluster (for the OpenSource version the replication setting has no effect).\nMeasurements are the “data structure” used to model how the data is stored in Influx and the fields associated with it.\nFields represent the actual data value. These are required and, something really important, they are not indexed.\nTags represents metadata associated with the aforementioned fields. They are optional, but they are very useful because they are indexed and allow you to perform group by and/or filter operations (you can filter on fields as well, but since they are not indexed, this is not a performant operation).\nSeries are a logical grouping of data defined by a measurement, a set of tags and a field. To me, this is one of the most important concepts that need to be clear while working with InfluxDB, because many of the different concepts revolve around it.\nShards contains the actual encoded and compressed data. They are represented by a TSM file on disk (more on this later). Every shard contains a specific set of series so all the points falling on a given series will be stored in the same shard.\nIn order to get your data points inserted into the database, Influx defines a text-based protocol named line protocol.\nAn example of how a single data point looks like in the line protocol and how it maps to the concepts described before is shown in the picture below:\nInfluxDB storage engine At the beginning of the post, I described some of the features that, per my understanding, the Influx folks were looking for when building their storage engine and how all these requirements led them to their current storage solution.\nTheir storage solution is similar to an LSM Tree and, from a high-level perspective, it is composed by:\n A write-ahead log A collection of TSM (read-only) files where the actual time series data is stored (similar to the SSTables) TSI files that serve as the inverted index used to quickly access the information. Prior to this version of the index, the data was held into an in-memory data structure, making it difficult to support high-cardinality scenarios (millions of series).  Before we move forward, a short note about how I plan to organize the contents. The original idea was to put everything I wanted to write down into a single post but it would be probably a little bit dense so I have decided to split it into 3 different parts:\n The current one with the introduction to Influx, the overview of the storage engine and a quick tour around the WAL component and its cache counterpart. The second one will cover the details about the TSM. The third one will cover details about the TSI.  For this post, let’s try to go a little bit deeper into the write ahead log (WAL) and its cache counterpart.\nWrite ahead log (WAL) The WAL is a write-optimized data structure that allows the writes to be durable and its main goal is to allow the writes to be appended as fast as possible so it is not easily queryable.\nWhen a new write comes into the system the new points are:\n Stored at an in-memory cache (more about this in the next section). Serialized, compressed using Snappy and written to the WAL.  If we take a look at the source code we can see where this actually happens:\n// WritePoints writes metadata and point data into the engine. // It returns an error if new points are added to an existing key. func (e *Engine) WritePoints(points []models.Point) error { …. // first try to write to the cache if err := e.Cache.WriteMulti(values); err != nil { return err } if e.WALEnabled { if _, err := e.WAL.WriteMulti(values); err != nil { return err } } return seriesErr }  Note that we could disable the WAL and, in this case, writes will only exist in the cache and could be lost if a cache snapshot has not happened.\nThe format used to describe every one of the entries appended to the WAL follows a type-length-value encoding scheme with a single byte representing the type of the entry being stored (write, delete or a range delete), a 4 byte uint32 representing the length of the compressed block, followed by the actual compressed data block.\nLooking again at the source code here we can see how the actual writing is performed:\n// Write writes entryType and the buffer containing compressed entry data. func (w *WALSegmentWriter) Write(entryType WalEntryType, compressed []byte) error { var buf [5]byte buf[0] = byte(entryType) binary.BigEndian.PutUint32(buf[1:5], uint32(len(compressed))) if _, err := w.bw.Write(buf[:]); err != nil { return err } if _, err := w.bw.Write(compressed); err != nil { return err } w.size += len(buf) + len(compressed) return nil }  Cache The cache component is an in-memory data structure that holds a copy of all the points persisted in the WAL. As we’ve already mentioned in the previous section, when a new write comes into the system the new points are stored in this cache.\nThe aforementioned points are indexed by the key which is formed by the measurement name, the tag set and the unique field key. If we go back to our previous example where we had a single write coming into the system:\ninfrastructure_metrics,server=server-1,container=container-1 cpu_usage=82  The key would be represented by something like this:\nThe points stored in the cache are not compressed and an upper bound can be set so we can prevent unexpected out of memory situations (if the upper bound limit is exceeded new writes coming into the system will be rejected) and prevent the database service to be unexpectedly restarted.\nWhen the number of elements in the cache reaches a certain lower bound (it’s configurable as well) a snapshot of the cache is triggered to a TSM file and the corresponding WAL segment files are removed.\nIf we take a quick look to the source code we can see the behavior described in the previous paragraph:\n// compactCache continually checks if the WAL cache should be written to disk. func (e *Engine) compactCache() { t := time.NewTicker(time.Second) defer t.Stop() for { e.mu.RLock() quit := e.snapDone e.mu.RUnlock() select { case \u0026lt;-quit: return case \u0026lt;-t.C: e.Cache.UpdateAge() if e.ShouldCompactCache(time.Now()) { start := time.Now() e.traceLogger.Info(\u0026quot;Compacting cache\u0026quot;, zap.String(\u0026quot;path\u0026quot;, e.path)) err := e.WriteSnapshot() if err != nil \u0026amp;\u0026amp; err != errCompactionsDisabled { e.logger.Info(\u0026quot;Error writing snapshot\u0026quot;, zap.Error(err)) atomic.AddInt64(\u0026amp;e.stats.CacheCompactionErrors, 1) } else { atomic.AddInt64(\u0026amp;e.stats.CacheCompactions, 1) } atomic.AddInt64(\u0026amp;e.stats.CacheCompactionDuration, time.Since(start).Nanoseconds()) } } } }  When a new read operation is received, the storage engine will merge data from the TSM files with the data stored in the cache (so you can read data that hasn’t been snapshotted into the TSM files yet). At query processing time, a copy of the data is done so any new write coming into the system won’t affect the results of any running query.\nConclusion This post has been a short intro to InfluxDB and a brief overview of its storage subsystem. In addition to the previous intros, we’ve covered a few details of the WAL + Cache storage subsystem elements. As I have already mentioned, my idea is to publish two more posts: one of them covering a few details of the TSM part and the other one going through the inner workings of the TSI component.\nOf course, I am aware this is just an introduction, and the devil is in the details, but I do hope this provides you some insights into a few database design concepts and how a concrete database applies them.\nThanks for reading.\n","date":1586873670,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586873670,"objectID":"2de89c6c15f5c879dbfd37fe4ddd6d38","permalink":"https://migue.github.io/post/quick-tour-influx-storage/","publishdate":"2020-04-14T15:14:30+01:00","relpermalink":"/post/quick-tour-influx-storage/","section":"post","summary":"Writing on the blog about some of the technical stuff I usually enjoy and/or work with is something that’s always in my plans but I never find the right time do it. Due to all this quarantine related stuff, both my sleeping habits and the kid’s are being somehow affected and, well, here I am, staring at a blank page trying to start writing about databases. And, well, since I don’t longer work for an international company I thought it would be a good idea to somehow practice my poor English skills.","tags":["databases"],"title":"InfluxDB storage subsystem: an introduction","type":"post"},{"authors":["Miguel Ángel Pastor Olivar"],"categories":["Computer Science","Programming"],"content":"I would like to share with you a bunch of the tech talks I have recently watched or some of the latest books I have enjoyed.\nDisclaimer: this list is just a set of resources I have gone through more or less \u0026ldquo;recently\u0026rdquo; about a few different topics I use to enjoy. It\u0026rsquo;s not my goal to create a \u0026ldquo;Best Talks/Papers/Whatever List\u0026rdquo;, just wanted to share with you all some of the things I have found interesting lately (all the resources are listed in no specific order).\nDisclaimer 2: by \u0026ldquo;lately\u0026rdquo; I mean the last ~1.5 years, so probably many of the contents linked below will be quite old for most of you (I have slowed down a lot on my work-related stuff, but this is a different story).\nIf you do like databases, systems engineering, cloud computing, runtimes and low-level programming, this list might contain some pointers to a book/talk you could enjoy.\nTalks Let\u0026rsquo;s Talk Locks! by Kavya Joshi An amazing talk about how locks are used at different places (Linux syscalls, Go programming language) and its performance implications.\nActually, any talk coming from Kavya is usually wonderful.\n Video EBtree — Design for a Scheduler and Use (Almost) Everywhere by Andjelko Iharos This talk goes into the evolution of HAProxy’s internals and how the created the EBTree data structure in order to manage active and suspended tasks within their scheduler (and how they ended up using it almost everywhere)\n Video Performance Matters by Emery Berger I simply love this talk. Emery goes through many of the different factors that, potentially, can affect performance on modern hardware: memory layout, instruction prefetching, branch prediction, \u0026hellip; and some surprising ones like env variables.\nDuring the talk, he presents the stabilizer tool which randomizes programs layouts during runtime and introduced coz , a causal profiler.\n Video PID Loops and the Art of Keeping Systems Stable by Colm MacCárthaigh I like Colm\u0026rsquo;s talks a lot. In this case, the talk goes through PID loops, control theory and how a bunch of AWS systems apply these design principles.\n Video Structured Concurrency by Roman Elizarov Great talk about the evolution of asynchronous APIS in different programming languages and platforms and how they have applied the structured concurrency concepts to the design of the Kotlin\u0026rsquo;s concurrency libraries.\n Video Reduce your storage costs with Transient Replication and Cheap Quorums by Alex Petrov Alex talks about Voting With Witnesses a replication schema which is used in Google Spanner and Megastore and has inspired Apache Cassandra\u0026rsquo;s Transient Replication and Cheap Quorums implementation\n Video ftrace: Where modifying a running kernel all started by Steven Rostedt If you\u0026rsquo;ve ever wondered how tracing works in the Linux kernel you should probably watch it. A highly technical talk but really well executed even an stupid like myself could understand it.\n Video Applicable and Achievable Formal Verification by Heidy Khlaaf Heidy provides a nice overview of a few verification tools and techniques deployed in the safety critical industry and shows how this could be adapted to your systems.\n Video Correctness proofs of distributed systems with Isabelle/HOL by Martin Kleppmann An extended version of the talk that Martin did at Strange Loop 2019 . He explores how Isabelle can be used to analyze algorithms used in distributed systems, and prove them correct.\n Video Books Database internals by Alex Petrov Alex has done an amazing job writing this book. You will find different topics: storage engines like BTrees and LSMs, how data is physically stored and the different building blocks involved, distributed systems and database clustering among many others\nIf you like databases and its internals this book is going to be a fun read.\n O\u0026rsquo;Reilly Serious Cryptography by Jean-Philippe Aumasson Another wonderful book. A practical guide to modern encryption that goes through the fundamental mathematical concepts at the heart of cryptography: authenticated encryption, hash functions, block ciphers, and public-key techniques (RSA and elliptic curve cryptography).\nTotally recommended if you want to learn a little bit more about cryptography, and, even if you\u0026rsquo;re a seasoned practitioner (not my case) I think you can learn a few things.\n O\u0026rsquo;Reilly Optimizing Java By Benjamin Evans, James Gough, and Chris Newland One more book I have enjoyed a lot. A practical approach to JVM performance tuning and how to identify and solve performance related issues. The book will help you to understand Java platform\u0026rsquo;s internals (if you\u0026rsquo;re willing to go through it :) )\n O\u0026rsquo;Reilly Efficient IO with io_uring I wasn\u0026rsquo;t sure where to put this resource; technically this is not a book, nor a paper either. Anyway, this is a gorgeous read about the newest Linux IO interface, io_uring and compare it to the current alternatives.\nWhy this work is being done, how this works, \u0026hellip; IO_URING is a huge step forward in the Linux kernel and this short introduction provides a gentle introduction\n IO_URING introduction Cloud Native Data Center Networking By Dinesh Dutt A really nice read about how modern cloud native data centers networks work and the steps required to design a datacenter that\u0026rsquo;s reliable and easy to manage. A mix of theory and practice which tries to guide the reader through everything which is needed to create and operate a network infrastructure in a modern datacenter.\nThe book covers many different topics: network disaggregation, routing protocols, network virtualization, \u0026hellip;\n O\u0026rsquo;Reilly BPF Performance Tools: Linux System and Application Observability by Brendan Gregg I am still going through it, but I am enjoying it so much I thought I should include it in this list. A really comprehensive guide about BPF tools, performance engineering, and kernel internals. The book includes tons of examples,tools, \u0026hellip;\n O\u0026rsquo;Reilly My original idea was to include a section with the papers I have recently enjoyed as well, but the post had become way too long that I decided to split it and do a follow-up including the academic papers.\n","date":1580739270,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580937917,"objectID":"9c2c1686e7451daafc6f8cfca9e4fb26","permalink":"https://migue.github.io/post/books-and-talks/","publishdate":"2020-02-03T15:14:30+01:00","relpermalink":"/post/books-and-talks/","section":"post","summary":"I would like to share with you a bunch of the tech talks I have recently watched or some of the latest books I have enjoyed.\nDisclaimer: this list is just a set of resources I have gone through more or less \u0026ldquo;recently\u0026rdquo; about a few different topics I use to enjoy. It\u0026rsquo;s not my goal to create a \u0026ldquo;Best Talks/Papers/Whatever List\u0026rdquo;, just wanted to share with you all some of the things I have found interesting lately (all the resources are listed in no specific order).","tags":["books","talks"],"title":"Books and Talks","type":"post"},{"authors":null,"categories":["Personal"],"content":"Siempre me he considerado una persona con suerte; con mucha suerte.\nHe tenido la suerte de tener unos padres que siempre se han preocupado de que tanto mi hermana como yo tuviéramos acceso a las oportunidades que ellos no pudieron tener, aunque eso implicara pasarse toda su vida trabajando como auténticos cabrones.\nSuerte por haber tenido amigos que han estado siempre a mi lado durante los buenos momentos y también durante los malos.\nSuerte por haber conocido a gente excepcional, a los que puedo llamar amigos, que me ha ayudado a navegar por el mundo profesional en el que me muevo y siempre ha tenido a bien aconsejarme y/o apoyarme en aquellos momentos en os que lo he necesitado.\nSuerte por haber conocido a la que hoy es mi mujer y con la que he formado una familia de la que me encuentro extremadamente orgulloso.\n¿Y por qué todo esto os estaréis preguntando? Soy consciente de que hay mucha gente a la que la suerte de la que hablaba anteriormente no le ha sonreido y me gustaría devolver un poco de la misma e intentar que el mundo en el que vivimos sea un poquito menos malo.\nCon el objetivo de aportar nuestro pequeño granito de arena, a principios de este año mi mujer y yo decidimos embarcarnos en el proceso necesario para formar parte del programa de Acogida Familiar de la Comunidad de Madrid (http://www.comunidad.madrid/servicios/asuntos-sociales/acogimiento-familiar-menores). El enlace anterior explica muy bien, y con todo nivel de detalle, en qué consiste el proceso y cuáles son los objetivos del mismo. Para el que no le apetezca leérselo, de manera resumida, se trata de un programa donde las familias se ofrecen a acoger a niños de manera temporal (existen diferentes modalidades) mientras sus padres no pueden hacerse cargo de ellos.\nDurante los últimos meses donde hemos tenido que realizar cursos y mantener entrevistas con diferentes psicólogos y trabajadores sociales. En muchos momentos ha sido un proceso \u0026ldquo;complicado\u0026rdquo;, desde una persepectiva emocional, no por la dureza de las entrevistas si no por las historias que rodean a muchos de los niños que se encuentran bajo el amparo y custodia de la Comunidad de Madrid.\nPues bien, el pasado viernes tuvimos la que se supone será la última entrevisa del proceso. Ahora mismo estamos pendientes de que las diferentes trabajoras sociales y psicólogas con las que nos hemos estado entrevistando emitan los informes correspondientes para que el pleno de la comisión del programa decida si podemos formar parte del mismo. Nos han adelantado que los informes son favorables y que vamos a ser aceptados en el programa (salvo catástrofe), aunque necesitamos el \u0026ldquo;sello\u0026rdquo; del pleno para ser parte oficial del mismo.\nA partir de que dicha notificación sea efectiva pasaríamos a ser parte oficial del programa, y, en \u0026ldquo;cualquier momento\u0026rdquo;, podrían avisarnos de que existe un niño/a que encaja en el ofrecimiento que nosotros hemos hecho como familia.\nMe resulta difícil expresar cómo me siento. Por una parte estoy muy contento de que nos hayan \u0026ldquo;aceptado\u0026rdquo; en el programa y de que podamos ayudar a gente que realmente lo necesita de una manera mucho más proactiva. Sin embargo, al mismo tiempo también me encuentro un poco \u0026ldquo;asustado\u0026rdquo; pensando en si seremos capaces de adaptarnos a esta nueva situación.\nVeremos que nos depara el futuro.\n","date":1568064661,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568064661,"objectID":"74a7accd4b6c6bdfed952687ef6413a6","permalink":"https://migue.github.io/post/suerte/","publishdate":"2019-09-09T22:31:01+01:00","relpermalink":"/post/suerte/","section":"post","summary":"Siempre me he considerado una persona con suerte; con mucha suerte.\nHe tenido la suerte de tener unos padres que siempre se han preocupado de que tanto mi hermana como yo tuviéramos acceso a las oportunidades que ellos no pudieron tener, aunque eso implicara pasarse toda su vida trabajando como auténticos cabrones.\nSuerte por haber tenido amigos que han estado siempre a mi lado durante los buenos momentos y también durante los malos.","tags":["news"],"title":"Suerte","type":"post"},{"authors":null,"categories":["Papers We Love"],"content":"Parece imposible que hayan pasado cuatro años desde que iniciásemos la andadura del Papers We Love Madrid .\nHan sido cuatro años con altibajos en los que nunca hemos llegado a tener la periodicidad y actividades que teníamos en la cabeza cuando se arrancó el grupo. Cuatro años en los que hemos \u0026ldquo;sufrido\u0026rdquo; mucho más de lo esperado en encontrar a gente que se animase a dar una charla, en los que hemos intentando generar los contenidos más diversos posibles aunque no siempre lo hayamos conseguido. Cuatro años en los que hemos sufrido, al igual que en otros muchos grupos, la falta de asistencia de las personas que inicialmente estaban inscritas.\nEl objetivo inicial era acercar los mundos académicos y empresariales y demostrar que ambos pueden beneficiarse el uno del otro. Lamentablemente, hemos fracasado.\nPero no todo son aspectos negativos; también nos hemos divertido mucho por el camino. Hemos tenido la suerte de contar con expertos internacionales en sus respectivas áreas: Carlos Baquero, Álvaro Videla o Christopher Meiklejohn entre otros muchos, hemos hablado de muchísimos temas interesantes: machine learning, sistemas distribuidos, protocolos, \u0026hellip; y hemos tenido la suerte de contar con miembros del grupo que son unos auténticos cracks, aunque nunca se hayan animado a dar una charla :)\nEn el aspecto más personal, me gustaría dar las gracias a mi compañero de batallas Félix sin el cual todo esto que hemos hecho, aunque sea poco, no hubiera sido posible.\nCon todo esto, y como la mayor parte ya habréis deducido a partir del título del post, muy a nuestro pesar, damos por cerrada nuestra actividad como organizadores del Papers We Love Madrid. Todos los recursos que hemos generado a lo largo de este periplo están disponibles en el canal oficial de Papers We Love ¡Muchas gracias a todos por vuestro apoyo y colaboración!\n¡Hasta pronto!\n","date":1546680661,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546680661,"objectID":"aa8c2cf47dbba5a37bd53fc2014117de","permalink":"https://migue.github.io/post/papers-we-love-madrid-final-chapter/","publishdate":"2019-01-05T10:31:01+01:00","relpermalink":"/post/papers-we-love-madrid-final-chapter/","section":"post","summary":"Parece imposible que hayan pasado cuatro años desde que iniciásemos la andadura del Papers We Love Madrid .\nHan sido cuatro años con altibajos en los que nunca hemos llegado a tener la periodicidad y actividades que teníamos en la cabeza cuando se arrancó el grupo. Cuatro años en los que hemos \u0026ldquo;sufrido\u0026rdquo; mucho más de lo esperado en encontrar a gente que se animase a dar una charla, en los que hemos intentando generar los contenidos más diversos posibles aunque no siempre lo hayamos conseguido.","tags":["news"],"title":"Papers We Love Madrid: capítulo final","type":"post"},{"authors":null,"categories":["Papers We Love"],"content":"Como seguramente alguno de vosotros recordais, hace unas semanas planteé por aquí la posibilidad de organizar un Papers We Love en Madrid. Unas cuantas personas mostraron su interés tanto en participar como en colaborar lo que me hizo seguir adelante con la iniciativa propuesta.\nMeetup Gracias a la inestimable participación del gran Féliz López hemos conseguido, bueno, realmente él ha conseguido que Álvaro Videla sea el ponente que de el pistoletazo de salida al nuevo grupo.\nParece que Álvaro tiene previsto pasarse por aquí en Febrero, con lo que \u0026ldquo;retrasaríamos\u0026rdquo; hasta ese momento la primera reunión del grupo. En cuanto al paper, todavía no he hablado con Álvaro, pero en cuanto me lo concrete, lo actualizaré tanto aquí como en la futura página de Meetup del grupo (véase más abajo)\nUbicación Todavía no tenemos una ubicación definitiva aunque varias personas se han ofrecido a través de Twitter a conseguir espacios donde podamos celebrar la primera reunión. Por simplicidad de las gestiones, había hablado con mi jefe sobre la posibilidad de disponer de las oficinas de Liferay España y la respuesta ha sido positiva. Del mismo modo, Félix me ha comentado que la nueva oficina de ShuttleCloud también es una opción viable. Por último, Francisco Fernández me comentaba que podría conseguir las oficinas de Tuenti .\nPersonalmente creo el abanico de opciones para esta primera charla es lo suficientemente amplio (y céntrico :D ). Cuando la ubicación sea definitiva, lo haré saber.\nGrupo de Meetup Dado que parece que todo esto va tomando forma, iré creando el grupo de Meetup además de revisar el resto de \u0026ldquo;requisitos\u0026rdquo; que establece la comunidad de Papers We Love\n¡Muchas gracias por vuestra atención!\nMigue\n","date":1417555061,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417555061,"objectID":"08ac2c3df3ebc1a8b2bbe6cd4070c8df","permalink":"https://migue.github.io/post/papers-we-love-madrid-tomando-forma/","publishdate":"2014-12-02T22:17:41+01:00","relpermalink":"/post/papers-we-love-madrid-tomando-forma/","section":"post","summary":"Como seguramente alguno de vosotros recordais, hace unas semanas planteé por aquí la posibilidad de organizar un Papers We Love en Madrid. Unas cuantas personas mostraron su interés tanto en participar como en colaborar lo que me hizo seguir adelante con la iniciativa propuesta.\nMeetup Gracias a la inestimable participación del gran Féliz López hemos conseguido, bueno, realmente él ha conseguido que Álvaro Videla sea el ponente que de el pistoletazo de salida al nuevo grupo.","tags":["news"],"title":"Papers We Love Madrid: Tomando Forma","type":"post"},{"authors":null,"categories":["Papers We Love"],"content":"Hace unos días, a raíz de un tweet sobre Julia , salió a la luz un tema al que hace tiempo le llevo dando bastantes vueltas: Papers we Love .\nLa comunidad Para todos aquellos que nunca han oído hablar de esta iniciativa, no es más que un repositorio de papers académicos en torno al cual se ha creado una comunidad que realizar charlas periódicas donde presentar y discutir sobre los mismos.\nLas normas No tiene mucho sentido que replique aquí todas las normas: aquí están detalladas.\nLos requisitos básicos son disponer de un grupo de Meetup y realizar reuniones razonablemente periódicas (para que ellos te mantengan en la lista de grupos activos).\nLa propuesta Seguro que muchos de vosotros os estaréis preguntando: ¿Por qué? ¿No existen suficientes grupos de usuarios en Madrid?\nSiempre he pensado que el mundo de la investigación y el mundo empresarial deberían estar mucho más cerca el uno del otro, y me gustaría que este grupo permitiera a los integrantes ver cómo ambos mundos pueden beneficiarse mutuamente.\nMe gustaría destacar que la temática de este grupo no está focalizada en una tecnología, plataforma o lenguaje de programación concretos sino que gira en torno a papers que los integrantes del grupo consideren interesantes (y, por su puesto, de la visión que el speaker le quiera dar :) ). Si os dais una vuelta por los meetups de otras ciudades veréis que las temáticas son de lo más dispares.\nEvidentemente esto no es más que una idea/propuesta, para ver si habría gente interesada en que se pudiera formar el grupo.\n¡Muchas gracias por vuestra atención!\nMigue\n","date":1415826941,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1415826941,"objectID":"4e6e11c20f0b82bd700950d6cc524b5d","permalink":"https://migue.github.io/post/papers-we-love-madrid-edition/","publishdate":"2014-11-12T22:15:41+01:00","relpermalink":"/post/papers-we-love-madrid-edition/","section":"post","summary":"Hace unos días, a raíz de un tweet sobre Julia , salió a la luz un tema al que hace tiempo le llevo dando bastantes vueltas: Papers we Love .\nLa comunidad Para todos aquellos que nunca han oído hablar de esta iniciativa, no es más que un repositorio de papers académicos en torno al cual se ha creado una comunidad que realizar charlas periódicas donde presentar y discutir sobre los mismos.","tags":["news"],"title":"Papers We Love Madrid Edition","type":"post"},{"authors":null,"categories":["programming"],"content":"Lately I have seen quite a few misconceptions regarding how string concatenation is handled in the Java world so I would like to write this short blog entry with a couple of stupid examples in order to show the basics of how it is done.\nNote: I am focusing here in the bytecode generated by the Java compiler, regardless any optimization the runtime could apply.\nConcatenating constant strings Imagine we are writing a new class where we have defined three final string fields and we want to add a new method which just returns the sum of the three:\npublic class StringConcatenation { public final String A = \u0026#34;A\u0026#34;, B= \u0026#34;B\u0026#34;, C= \u0026#34;C\u0026#34;; public String concatFinalStrings() { return A + B + C; } } Taking a look to the bytecode generated by the Java compiler we can see the following (I have included only the relevant parts of the command java -v for the sake of clarity):\nConstant pool: ... #9 = String #49 // ABC ... public java.lang.String concatFinalStrings(); descriptor: ()Ljava/lang/String; flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: ldc #9 // String ABC 2: areturn LineNumberTable } The previous snippet shows how the Java compiler will generate for us a new constant (position 9 of the constants pool) containing the concatentation of the three strings, and the implementation of the method is just an ldc instruction. As far as I remember, the JLS specifies that compile time constants should end up in an internted string\nFor the reader: remove the final modifier at the fields declaration and analyze the generated bytecode. Do you see any difference?\nConcatenating non constant strings The example shown at the previous section could be very unrealistic for many of you so let\u0026rsquo;s see if we can get something more interesting in place. Now we are planning to add a new method which just invokes three other methods and links together the results , like this:\npublic class StringConcatenation { public String concatVariableStrings() { return getA() + getB() + getC(); } public String getA() { return \u0026#34;A\u0026#34;; } public String getB() { return \u0026#34;B\u0026#34;; } public String getC() { return \u0026#34;C\u0026#34;; } } Taking a closer look at the generated bytecode (again just included the most important sections):\npublic java.lang.String concatVariableStrings(); descriptor: ()Ljava/lang/String; flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: new #8 // class java/lang/StringBuilder 3: dup 4: invokespecial #9 // Method java/lang/StringBuilder.\"\":()V 7: aload_0 8: invokevirtual #12 // Method getA:()Ljava/lang/String; 11: invokevirtual #10 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 14: aload_0 15: invokevirtual #13 // Method getB:()Ljava/lang/String; 18: invokevirtual #10 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 21: aload_0 22: invokevirtual #14 // Method getC:()Ljava/lang/String; 25: invokevirtual #10 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 28: invokevirtual #11 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 31: areturn LineNumberTable: line 12: 0 we can see the Java compiler is creating a StringBuilder under the hood, so no temporary String objects are allocated in order to compose the final string which needs to be returned from the method. If you need to improve the performance, you should write the builder by your own, setting the initial size in the constructor.\nMaybe this is not new for you (as it should be), but, lately, I have received quite a few questions regarding this behaviour.\nEnvironment: I have used javac and javap 1.8.0_05 in OSx to compile the examples.\nFor the curious If you \u0026ldquo;port\u0026rdquo; the previous examples to Scala and take a look to the generated bytecode (of the second example), you will see that the Scala compiler will use the same approach shown before; generating a StringBuilder (scala.collection.mutable.StringBuilder) under the covers.\n","date":1398115746,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398115746,"objectID":"ba40284162135dd1dac91821fa8acc20","permalink":"https://migue.github.io/post/java-compiler-string-addition/","publishdate":"2014-04-21T22:29:06+01:00","relpermalink":"/post/java-compiler-string-addition/","section":"post","summary":"Lately I have seen quite a few misconceptions regarding how string concatenation is handled in the Java world so I would like to write this short blog entry with a couple of stupid examples in order to show the basics of how it is done.\nNote: I am focusing here in the bytecode generated by the Java compiler, regardless any optimization the runtime could apply.\nConcatenating constant strings Imagine we are writing a new class where we have defined three final string fields and we want to add a new method which just returns the sum of the three:","tags":["jvm","compilers"],"title":"Java Compiler String Addition","type":"post"},{"authors":null,"categories":["personal"],"content":"About two weeks ago I got a surprisingly email in my inbox. An old friend of mine is about to launch a new startup and he asked me to become the CTO of their new company. My first reaction was: Wow!\nSince this first email we have talked each other (a colleague of him was involved too) quite a few times. They have a good idea, an already working prototype of the product they want to create and enough money to be alive for a while if things don\u0026rsquo;t go as expected. I have thinking a lot about the pros and cons of joining them on their new adventure and I have finally made a decision: I am not joining them.\nAs I have already written at the beginning of the post, their new project has a bunch of attractive things: from a pure technology perspective their product will require some of the newest technologies in order to solve their problem; I would say it is a \u0026ldquo;fun\u0026rdquo; problem from a techical perspective. From a business perspective they already have quite a few good ideas, I am just curious on how they finally will set the main target audience of the product. So, I guess, your next question will be, why aren\u0026rsquo;t you joining them? I have a really short and concise answer: my family.\nCreating a new startup is a really hard and time consuming task, even with the above-described attenuating. Tons of effort are required to get everything in the right place: finding new clients, traveling, developing the product, hiring people, compose a new team and so on. All of this stuff is really interesting, and, don\u0026rsquo;t take me wrong , I am a pretty hard worker and love my job (all of those who already know me could corroborate that), but, right now, my top priority is to spend as much time as possible with my wife and my son (we are planning to increase the family in the near future (hopefully :)), and I think all this stuff isn\u0026rsquo;t too much compatible with becoming a CTO of a new startup at this moment. Of course, all of this is just my personal opinions based on my own experiences (and my friends\u0026rsquo;).\nI don\u0026rsquo;t know if I am making the right decision, but I am being consistent with my priorities. I have been completely transparent with them, and I think they have understood my final choice; it has been a considered decision.\nBesides my personal reasons, to be honest, I am not sure if I would be able to successfully achieve all the things a CTO is supposed to undertake. I am just a mere technical guy who tries to learn as much as possible of all the people around him, and that, every single day, realize that he has no real idea of how computers work :).\nI\u0026rsquo;m not entirely convinced I have expressed myself correctly because of the language (this is one of the main reasons I switched the blog\u0026rsquo;s language, just to force me and try to improve my skills) but I wanted to write it down and share with all of you. I like to hear about this kind of stories: the reasons why people start a new company, decline a good job offer or cross the whole world looking for new challenges.\nI am sure my friends are going to succeed on their new adventure. I would wish them luck, but I\u0026rsquo;m completely sure they don\u0026rsquo;t need it.\n","date":1390728970,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1390728970,"objectID":"20f63167144d25180a5362eedbe658e2","permalink":"https://migue.github.io/post/saying-no/","publishdate":"2014-01-26T10:36:10+01:00","relpermalink":"/post/saying-no/","section":"post","summary":"About two weeks ago I got a surprisingly email in my inbox. An old friend of mine is about to launch a new startup and he asked me to become the CTO of their new company. My first reaction was: Wow!\nSince this first email we have talked each other (a colleague of him was involved too) quite a few times. They have a good idea, an already working prototype of the product they want to create and enough money to be alive for a while if things don\u0026rsquo;t go as expected.","tags":["jobs",""],"title":"Saying No","type":"post"}]