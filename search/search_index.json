{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes: an intro for application developers The goal of this course is to teach the students what Kubernetes is and how it can help them to easily deploy and operate their applications. We will learn how to leverage Kubernetes' capabilities to put into practice all the different Cloud Native concepts we've explored along the subject.","title":"Home"},{"location":"#kubernetes-an-intro-for-application-developers","text":"The goal of this course is to teach the students what Kubernetes is and how it can help them to easily deploy and operate their applications. We will learn how to leverage Kubernetes' capabilities to put into practice all the different Cloud Native concepts we've explored along the subject.","title":"Kubernetes: an intro for application developers"},{"location":"about/","text":"Personal information My name is Miguel \u00c1ngel Pastor Olivar, a proud dad and husband, who happens to work in the software world. Passionate reader, chef aficionado, former surf player and current cyclist and runner. I am unsuccessfully pursuing to move my Phd research forward. I am currently working as a Staff Engineer at GitHub as a member of their platform and infrastructure team. Email You can reach me at mapastorol@upsa.es Twitter I use Twitter more than I should. You will find tech related stuff and some ocasional rants about a few different topics here . GitHub In my GitHub profile you can find what things I\u2019ve been working on (and the source code of this webpage as well).","title":"About"},{"location":"about/#personal-information","text":"My name is Miguel \u00c1ngel Pastor Olivar, a proud dad and husband, who happens to work in the software world. Passionate reader, chef aficionado, former surf player and current cyclist and runner. I am unsuccessfully pursuing to move my Phd research forward. I am currently working as a Staff Engineer at GitHub as a member of their platform and infrastructure team.","title":"Personal information"},{"location":"about/#email","text":"You can reach me at mapastorol@upsa.es","title":"Email"},{"location":"about/#twitter","text":"I use Twitter more than I should. You will find tech related stuff and some ocasional rants about a few different topics here .","title":"Twitter"},{"location":"about/#github","text":"In my GitHub profile you can find what things I\u2019ve been working on (and the source code of this webpage as well).","title":"GitHub"},{"location":"deploying/","text":"Deploying a Kubernetes cluster Before we can deploy our application into a Kubernetes cluster we need to have one running. There're many different alternatives, ranging from installing our own cluster from scratch, using a tool like K3S or using one of the multiple options provided by the different public cloud providers. It's not in our goals to learn how to install and operate a Kubernetes cluster but to learn how to deploy and manage our applications in it, so we will use the public cloud provider route. During the previous topics in our subject, AWS has been the public cloud provider of our choice, and, aiming to open our horizons, we will shift a bit and will use the Kubernetes engine service provided by the Google Cloud Platform . Let's get started: Installing the Cloud SDK The first thing we will need to do is to install the Cloud SDK which includes the gcloud command line we will be using during our examples. You can find the installation instructions in the previous link, but, if you're in a Mac you can just run: brew install --cask google-cloud-sdk Creating our first Kubernetes cluster Now that we've the required software available the first thing we need to do is to authenticate us: gcloud auth login This will open a browser and will let you login using your Google account. Right after you've successfully logged in you should see a message like the following one: You are now logged in as [miguelinlas3@gmail.com]. Your current project is [None]. You can change this setting by running: $ gcloud config set project PROJECT_ID Following the previous instructions, we can set the default projectin our tooling; in my case: gcloud config set project kubernetes-intro-2021-2022 Now, we could list the set of Kubernetes clusters we have available: gcloud container clusters list We should get an empty list back, because we haven't created anything yet. Let's fix it. gcloud container clusters create mimo-2020-2021 --zone europe-west1-b This action will take a few minutes and once it's ready we will be able to retrieve the credentials for the cluster using a command like: gcloud auth application-default login Exploring our Kubernetes cluster using Kubectl At this point we're ready to explore our Kubernetes clusters using the kubectl command line: kubectl version Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.5\", GitCommit:\"aea7bbadd2fc0cd689de94a54e5b7b758869d691\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:10:45Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.5-gke.1302\", GitCommit:\"639f3a74abf258418493e9b75f2f98a08da29733\", GitTreeState:\"clean\", BuildDate:\"2021-10-21T21:35:48Z\", GoVersion:\"go1.16.7b7\", Compiler:\"gc\", Platform:\"linux/amd64\"} According to the previous output it seems we're able to take with the Kubernetes cluster we created in the previous step, so,let's get started and check an overview of our cluster's status: kubectl get componentstatuses Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR etcd-1 Healthy {\"health\":\"true\"} etcd-0 Healthy {\"health\":\"true\"} scheduler Healthy ok controller-manager Healthy ok In the previous output we can see the major components that make up our Kubernetes cluster, all of them looking healthy. Let's retrieve now the list of nodes in our cluster: kubectl get nodes NAME STATUS ROLES AGE VERSION gke-mimo-2020-2021-default-pool-04826909-61l1 Ready <none> 6m30s v1.21.5-gke.1302 gke-mimo-2020-2021-default-pool-04826909-n2sh Ready <none> 6m29s v1.21.5-gke.1302 gke-mimo-2020-2021-default-pool-04826909-zpzd Ready <none> 6m31s v1.21.5-gke.1302 and let get the details for a particular one: kubectl describe nodes gke-mimo-2020-2021-default-pool-04826909-61l1 Name: gke-mimo-2020-2021-default-pool-04826909-61l1 Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=e2-medium beta.kubernetes.io/os=linux cloud.google.com/gke-boot-disk=pd-standard cloud.google.com/gke-container-runtime=containerd cloud.google.com/gke-nodepool=default-pool cloud.google.com/gke-os-distribution=cos cloud.google.com/machine-family=e2 failure-domain.beta.kubernetes.io/region=europe-west1 failure-domain.beta.kubernetes.io/zone=europe-west1-b kubernetes.io/arch=amd64 kubernetes.io/hostname=gke-mimo-2020-2021-default-pool-04826909-61l1 kubernetes.io/os=linux node.kubernetes.io/instance-type=e2-medium topology.gke.io/zone=europe-west1-b topology.kubernetes.io/region=europe-west1 topology.kubernetes.io/zone=europe-west1-b Annotations: container.googleapis.com/instance_id: 6099595287198508223 csi.volume.kubernetes.io/nodeid: {\"pd.csi.storage.gke.io\":\"projects/kubernetes-intro-2021-2022/zones/europe-west1-b/instances/gke-mimo-2020-2021-default-pool-04826909-61l1... node.alpha.kubernetes.io/ttl: 0 node.gke.io/last-applied-node-labels: cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cl... volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Wed, 08 Dec 2021 12:45:24 +0100 Taints: <none> Unschedulable: false Lease: HolderIdentity: gke-mimo-2020-2021-default-pool-04826909-61l1 AcquireTime: <unset> RenewTime: Wed, 08 Dec 2021 12:52:50 +0100 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- FrequentContainerdRestart False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentContainerdRestart containerd is functioning properly FrequentUnregisterNetDevice False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentUnregisterNetDevice node is functioning properly KernelDeadlock False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 KernelHasNoDeadlock kernel has no deadlock ReadonlyFilesystem False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 FilesystemIsNotReadOnly Filesystem is not read-only CorruptDockerOverlay2 False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoCorruptDockerOverlay2 docker overlay2 is functioning properly FrequentKubeletRestart False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentKubeletRestart kubelet is functioning properly FrequentDockerRestart False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentDockerRestart docker is functioning properly NetworkUnavailable False Wed, 08 Dec 2021 12:45:25 +0100 Wed, 08 Dec 2021 12:45:25 +0100 RouteCreated NodeController create implicit route MemoryPressure False Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:09 +0100 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:09 +0100 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:09 +0100 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:25 +0100 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 10.132.0.3 ExternalIP: 104.199.30.192 InternalDNS: gke-mimo-2020-2021-default-pool-04826909-61l1.europe-west1-b.c.kubernetes-intro-2021-2022.internal Hostname: gke-mimo-2020-2021-default-pool-04826909-61l1.europe-west1-b.c.kubernetes-intro-2021-2022.internal Capacity: attachable-volumes-gce-pd: 15 cpu: 2 ephemeral-storage: 98868448Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 4031620Ki pods: 110 Allocatable: attachable-volumes-gce-pd: 15 cpu: 940m ephemeral-storage: 47093746742 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 2885764Ki pods: 110 System Info: Machine ID: 8e4c357db077e5b1567810eaff3c89bf System UUID: 8e4c357d-b077-e5b1-5678-10eaff3c89bf Boot ID: c2adf65e-6a32-410b-94a7-a6ce0c0b738a Kernel Version: 5.4.144+ OS Image: Container-Optimized OS from Google Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.4.8 Kubelet Version: v1.21.5-gke.1302 Kube-Proxy Version: v1.21.5-gke.1302 PodCIDR: 10.12.1.0/24 PodCIDRs: 10.12.1.0/24 ProviderID: gce://kubernetes-intro-2021-2022/europe-west1-b/gke-mimo-2020-2021-default-pool-04826909-61l1 Non-terminated Pods: (10 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- kube-system event-exporter-gke-5479fd58c8-qlw78 0 (0%) 0 (0%) 0 (0%) 0 (0%) 7m55s kube-system fluentbit-gke-m755p 100m (10%) 0 (0%) 200Mi (7%) 500Mi (17%) 7m33s kube-system gke-metrics-agent-79f5w 3m (0%) 0 (0%) 50Mi (1%) 50Mi (1%) 7m34s kube-system konnectivity-agent-5d5dc46d74-lkwhl 10m (1%) 0 (0%) 30Mi (1%) 30Mi (1%) 7m18s kube-system konnectivity-agent-autoscaler-5c49cb58bb-7nxx4 10m (1%) 0 (0%) 10M (0%) 0 (0%) 7m43s kube-system kube-dns-697dc8fc8b-kd68s 260m (27%) 0 (0%) 110Mi (3%) 210Mi (7%) 7m27s kube-system kube-dns-autoscaler-844c9d9448-cmm8m 20m (2%) 0 (0%) 10Mi (0%) 0 (0%) 7m44s kube-system kube-proxy-gke-mimo-2020-2021-default-pool-04826909-61l1 100m (10%) 0 (0%) 0 (0%) 0 (0%) 6m18s kube-system l7-default-backend-865b4c8f8b-qntdj 10m (1%) 0 (0%) 20Mi (0%) 0 (0%) 7m54s kube-system pdcsi-node-6rzcz 10m (1%) 0 (0%) 20Mi (0%) 100Mi (3%) 7m34s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 523m (55%) 0 (0%) memory 471373440 (15%) 890Mi (31%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-gce-pd 0 0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 9m1s kubelet Starting kubelet. Warning InvalidDiskCapacity 9m1s kubelet invalid capacity 0 on image filesystem Normal NodeAllocatableEnforced 9m1s kubelet Updated Node Allocatable limit across pods Normal NodeHasSufficientMemory 8m57s (x8 over 9m1s) kubelet Node gke-mimo-2020-2021-default-pool-04826909-61l1 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 8m57s (x7 over 9m1s) kubelet Node gke-mimo-2020-2021-default-pool-04826909-61l1 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 8m57s (x7 over 9m1s) kubelet Node gke-mimo-2020-2021-default-pool-04826909-61l1 status is now: NodeHasSufficientPID Warning ContainerdStart 7m35s (x2 over 7m35s) systemd-monitor Starting containerd container runtime... Warning DockerStart 7m35s (x3 over 7m35s) systemd-monitor Starting Docker Application Container Engine... Warning KubeletStart 7m35s systemd-monitor Started Kubernetes kubelet. Normal Starting 7m34s kube-proxy Starting kube-proxy. In the previous output we can find the type of operating system that the node is running on, the type of machine, the capacity, ... Closing thoughts We've created a Kubernetes cluster using a public cloud provider and we've briefly interacted with it; at this point we're ready to start playing with our application and try to get it deployed into our previous cluster.","title":"Deploying a Kubernetes cluster"},{"location":"deploying/#deploying-a-kubernetes-cluster","text":"Before we can deploy our application into a Kubernetes cluster we need to have one running. There're many different alternatives, ranging from installing our own cluster from scratch, using a tool like K3S or using one of the multiple options provided by the different public cloud providers. It's not in our goals to learn how to install and operate a Kubernetes cluster but to learn how to deploy and manage our applications in it, so we will use the public cloud provider route. During the previous topics in our subject, AWS has been the public cloud provider of our choice, and, aiming to open our horizons, we will shift a bit and will use the Kubernetes engine service provided by the Google Cloud Platform . Let's get started:","title":"Deploying a Kubernetes cluster"},{"location":"deploying/#installing-the-cloud-sdk","text":"The first thing we will need to do is to install the Cloud SDK which includes the gcloud command line we will be using during our examples. You can find the installation instructions in the previous link, but, if you're in a Mac you can just run: brew install --cask google-cloud-sdk","title":"Installing the Cloud SDK"},{"location":"deploying/#creating-our-first-kubernetes-cluster","text":"Now that we've the required software available the first thing we need to do is to authenticate us: gcloud auth login This will open a browser and will let you login using your Google account. Right after you've successfully logged in you should see a message like the following one: You are now logged in as [miguelinlas3@gmail.com]. Your current project is [None]. You can change this setting by running: $ gcloud config set project PROJECT_ID Following the previous instructions, we can set the default projectin our tooling; in my case: gcloud config set project kubernetes-intro-2021-2022 Now, we could list the set of Kubernetes clusters we have available: gcloud container clusters list We should get an empty list back, because we haven't created anything yet. Let's fix it. gcloud container clusters create mimo-2020-2021 --zone europe-west1-b This action will take a few minutes and once it's ready we will be able to retrieve the credentials for the cluster using a command like: gcloud auth application-default login","title":"Creating our first Kubernetes cluster"},{"location":"deploying/#exploring-our-kubernetes-cluster-using-kubectl","text":"At this point we're ready to explore our Kubernetes clusters using the kubectl command line: kubectl version Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.5\", GitCommit:\"aea7bbadd2fc0cd689de94a54e5b7b758869d691\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:10:45Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.5-gke.1302\", GitCommit:\"639f3a74abf258418493e9b75f2f98a08da29733\", GitTreeState:\"clean\", BuildDate:\"2021-10-21T21:35:48Z\", GoVersion:\"go1.16.7b7\", Compiler:\"gc\", Platform:\"linux/amd64\"} According to the previous output it seems we're able to take with the Kubernetes cluster we created in the previous step, so,let's get started and check an overview of our cluster's status: kubectl get componentstatuses Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR etcd-1 Healthy {\"health\":\"true\"} etcd-0 Healthy {\"health\":\"true\"} scheduler Healthy ok controller-manager Healthy ok In the previous output we can see the major components that make up our Kubernetes cluster, all of them looking healthy. Let's retrieve now the list of nodes in our cluster: kubectl get nodes NAME STATUS ROLES AGE VERSION gke-mimo-2020-2021-default-pool-04826909-61l1 Ready <none> 6m30s v1.21.5-gke.1302 gke-mimo-2020-2021-default-pool-04826909-n2sh Ready <none> 6m29s v1.21.5-gke.1302 gke-mimo-2020-2021-default-pool-04826909-zpzd Ready <none> 6m31s v1.21.5-gke.1302 and let get the details for a particular one: kubectl describe nodes gke-mimo-2020-2021-default-pool-04826909-61l1 Name: gke-mimo-2020-2021-default-pool-04826909-61l1 Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=e2-medium beta.kubernetes.io/os=linux cloud.google.com/gke-boot-disk=pd-standard cloud.google.com/gke-container-runtime=containerd cloud.google.com/gke-nodepool=default-pool cloud.google.com/gke-os-distribution=cos cloud.google.com/machine-family=e2 failure-domain.beta.kubernetes.io/region=europe-west1 failure-domain.beta.kubernetes.io/zone=europe-west1-b kubernetes.io/arch=amd64 kubernetes.io/hostname=gke-mimo-2020-2021-default-pool-04826909-61l1 kubernetes.io/os=linux node.kubernetes.io/instance-type=e2-medium topology.gke.io/zone=europe-west1-b topology.kubernetes.io/region=europe-west1 topology.kubernetes.io/zone=europe-west1-b Annotations: container.googleapis.com/instance_id: 6099595287198508223 csi.volume.kubernetes.io/nodeid: {\"pd.csi.storage.gke.io\":\"projects/kubernetes-intro-2021-2022/zones/europe-west1-b/instances/gke-mimo-2020-2021-default-pool-04826909-61l1... node.alpha.kubernetes.io/ttl: 0 node.gke.io/last-applied-node-labels: cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cl... volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Wed, 08 Dec 2021 12:45:24 +0100 Taints: <none> Unschedulable: false Lease: HolderIdentity: gke-mimo-2020-2021-default-pool-04826909-61l1 AcquireTime: <unset> RenewTime: Wed, 08 Dec 2021 12:52:50 +0100 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- FrequentContainerdRestart False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentContainerdRestart containerd is functioning properly FrequentUnregisterNetDevice False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentUnregisterNetDevice node is functioning properly KernelDeadlock False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 KernelHasNoDeadlock kernel has no deadlock ReadonlyFilesystem False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 FilesystemIsNotReadOnly Filesystem is not read-only CorruptDockerOverlay2 False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoCorruptDockerOverlay2 docker overlay2 is functioning properly FrequentKubeletRestart False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentKubeletRestart kubelet is functioning properly FrequentDockerRestart False Wed, 08 Dec 2021 12:50:26 +0100 Wed, 08 Dec 2021 12:45:24 +0100 NoFrequentDockerRestart docker is functioning properly NetworkUnavailable False Wed, 08 Dec 2021 12:45:25 +0100 Wed, 08 Dec 2021 12:45:25 +0100 RouteCreated NodeController create implicit route MemoryPressure False Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:09 +0100 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:09 +0100 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:09 +0100 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Wed, 08 Dec 2021 12:51:36 +0100 Wed, 08 Dec 2021 12:45:25 +0100 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 10.132.0.3 ExternalIP: 104.199.30.192 InternalDNS: gke-mimo-2020-2021-default-pool-04826909-61l1.europe-west1-b.c.kubernetes-intro-2021-2022.internal Hostname: gke-mimo-2020-2021-default-pool-04826909-61l1.europe-west1-b.c.kubernetes-intro-2021-2022.internal Capacity: attachable-volumes-gce-pd: 15 cpu: 2 ephemeral-storage: 98868448Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 4031620Ki pods: 110 Allocatable: attachable-volumes-gce-pd: 15 cpu: 940m ephemeral-storage: 47093746742 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 2885764Ki pods: 110 System Info: Machine ID: 8e4c357db077e5b1567810eaff3c89bf System UUID: 8e4c357d-b077-e5b1-5678-10eaff3c89bf Boot ID: c2adf65e-6a32-410b-94a7-a6ce0c0b738a Kernel Version: 5.4.144+ OS Image: Container-Optimized OS from Google Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.4.8 Kubelet Version: v1.21.5-gke.1302 Kube-Proxy Version: v1.21.5-gke.1302 PodCIDR: 10.12.1.0/24 PodCIDRs: 10.12.1.0/24 ProviderID: gce://kubernetes-intro-2021-2022/europe-west1-b/gke-mimo-2020-2021-default-pool-04826909-61l1 Non-terminated Pods: (10 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- kube-system event-exporter-gke-5479fd58c8-qlw78 0 (0%) 0 (0%) 0 (0%) 0 (0%) 7m55s kube-system fluentbit-gke-m755p 100m (10%) 0 (0%) 200Mi (7%) 500Mi (17%) 7m33s kube-system gke-metrics-agent-79f5w 3m (0%) 0 (0%) 50Mi (1%) 50Mi (1%) 7m34s kube-system konnectivity-agent-5d5dc46d74-lkwhl 10m (1%) 0 (0%) 30Mi (1%) 30Mi (1%) 7m18s kube-system konnectivity-agent-autoscaler-5c49cb58bb-7nxx4 10m (1%) 0 (0%) 10M (0%) 0 (0%) 7m43s kube-system kube-dns-697dc8fc8b-kd68s 260m (27%) 0 (0%) 110Mi (3%) 210Mi (7%) 7m27s kube-system kube-dns-autoscaler-844c9d9448-cmm8m 20m (2%) 0 (0%) 10Mi (0%) 0 (0%) 7m44s kube-system kube-proxy-gke-mimo-2020-2021-default-pool-04826909-61l1 100m (10%) 0 (0%) 0 (0%) 0 (0%) 6m18s kube-system l7-default-backend-865b4c8f8b-qntdj 10m (1%) 0 (0%) 20Mi (0%) 0 (0%) 7m54s kube-system pdcsi-node-6rzcz 10m (1%) 0 (0%) 20Mi (0%) 100Mi (3%) 7m34s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 523m (55%) 0 (0%) memory 471373440 (15%) 890Mi (31%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-gce-pd 0 0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 9m1s kubelet Starting kubelet. Warning InvalidDiskCapacity 9m1s kubelet invalid capacity 0 on image filesystem Normal NodeAllocatableEnforced 9m1s kubelet Updated Node Allocatable limit across pods Normal NodeHasSufficientMemory 8m57s (x8 over 9m1s) kubelet Node gke-mimo-2020-2021-default-pool-04826909-61l1 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 8m57s (x7 over 9m1s) kubelet Node gke-mimo-2020-2021-default-pool-04826909-61l1 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 8m57s (x7 over 9m1s) kubelet Node gke-mimo-2020-2021-default-pool-04826909-61l1 status is now: NodeHasSufficientPID Warning ContainerdStart 7m35s (x2 over 7m35s) systemd-monitor Starting containerd container runtime... Warning DockerStart 7m35s (x3 over 7m35s) systemd-monitor Starting Docker Application Container Engine... Warning KubeletStart 7m35s systemd-monitor Started Kubernetes kubelet. Normal Starting 7m34s kube-proxy Starting kube-proxy. In the previous output we can find the type of operating system that the node is running on, the type of machine, the capacity, ...","title":"Exploring our Kubernetes cluster using Kubectl"},{"location":"deploying/#closing-thoughts","text":"We've created a Kubernetes cluster using a public cloud provider and we've briefly interacted with it; at this point we're ready to start playing with our application and try to get it deployed into our previous cluster.","title":"Closing thoughts"},{"location":"intro/","text":"Introduction Along the course we've learned what's the Cloud, how to develop Cloud Native applications and its corresponding foundations or how to operate and deploy applications using modern paradigms. We've briefly explored the platform as a service world and we've architected and deployed our applications using an infrastructure as a service provider. During this part of the course, we are going to get familiar with the container and container orchestrator world. We're going to follow a practical approach on this, and will try to put use a hands-on approach and put into practice everything we've learned. Our goal is similar to the ones from our previous topics: learn how to develop, deploy and operate cloud-native applications using these type of tools, and learn when these types of systems can be useful. It's not our goal to learn every single command, or the internals of the systems we will be using, as we've alredy mentioned in the previous paragraph, our goal is to learn the pros and cons of different approachs so we can decide what approach could be the best depending on the requirements. Container orchestrators In our development environments we usually run the containers in a single host, but, if we want to move our application/service to a production environment this is not a viable alternative. As we've seen during the whole subject, we would like to build a scalable and reliable system. A container orchestractor is a tool that glues systems altogeher, providing a unified deployment and management control plane, and, at the same time, meeting all the requirements mentioned before. There're different orchestrator available: Kubernetes , Nomad , Marathon , Swarm . Kubernetes is probably the most widely used and the one we will be using in our examples. Kubernetes Kubernetes , also known as K8s, is an open source system for managing containerized applications across multiple hosts. It provides basic mechanisms for deployment, maintenance, and scaling of applications. Kubernetes builds upon a decade and a half of experience at Google running production workloads at scale using a system called Borg , combined with best-of-breed ideas and practices from the community. Kubernetes's goal was to radically fight the way applications were built and deployed in Cloud environments, providing the developers more autonomy and velocity to perform their deployments. What are we going to do During this course we will learn how: What a container is and how to create one Running the previous container Deploy a Kubernetes cluster using a public Cloud provider: we are not going to install a Kubernetes cluster but use one of the managed versions provided by Google. Deploy an application and, along the way, learn the most important concepts that need to be taken into account if we want to create a robust, fault-tolerant and scalable system.","title":"Intro"},{"location":"intro/#introduction","text":"Along the course we've learned what's the Cloud, how to develop Cloud Native applications and its corresponding foundations or how to operate and deploy applications using modern paradigms. We've briefly explored the platform as a service world and we've architected and deployed our applications using an infrastructure as a service provider. During this part of the course, we are going to get familiar with the container and container orchestrator world. We're going to follow a practical approach on this, and will try to put use a hands-on approach and put into practice everything we've learned. Our goal is similar to the ones from our previous topics: learn how to develop, deploy and operate cloud-native applications using these type of tools, and learn when these types of systems can be useful. It's not our goal to learn every single command, or the internals of the systems we will be using, as we've alredy mentioned in the previous paragraph, our goal is to learn the pros and cons of different approachs so we can decide what approach could be the best depending on the requirements.","title":"Introduction"},{"location":"intro/#container-orchestrators","text":"In our development environments we usually run the containers in a single host, but, if we want to move our application/service to a production environment this is not a viable alternative. As we've seen during the whole subject, we would like to build a scalable and reliable system. A container orchestractor is a tool that glues systems altogeher, providing a unified deployment and management control plane, and, at the same time, meeting all the requirements mentioned before. There're different orchestrator available: Kubernetes , Nomad , Marathon , Swarm . Kubernetes is probably the most widely used and the one we will be using in our examples.","title":"Container orchestrators"},{"location":"intro/#kubernetes","text":"Kubernetes , also known as K8s, is an open source system for managing containerized applications across multiple hosts. It provides basic mechanisms for deployment, maintenance, and scaling of applications. Kubernetes builds upon a decade and a half of experience at Google running production workloads at scale using a system called Borg , combined with best-of-breed ideas and practices from the community. Kubernetes's goal was to radically fight the way applications were built and deployed in Cloud environments, providing the developers more autonomy and velocity to perform their deployments.","title":"Kubernetes"},{"location":"intro/#what-are-we-going-to-do","text":"During this course we will learn how: What a container is and how to create one Running the previous container Deploy a Kubernetes cluster using a public Cloud provider: we are not going to install a Kubernetes cluster but use one of the managed versions provided by Google. Deploy an application and, along the way, learn the most important concepts that need to be taken into account if we want to create a robust, fault-tolerant and scalable system.","title":"What are we going to do"},{"location":"load-balancing/","text":"Deploying our application: pods","title":"Load balancing"},{"location":"load-balancing/#deploying-our-application-pods","text":"","title":"Deploying our application: pods"},{"location":"pods/","text":"Deploying our application: pods","title":"Pods"},{"location":"pods/#deploying-our-application-pods","text":"","title":"Deploying our application: pods"},{"location":"replication/","text":"Deploying our application: pods","title":"Replication"},{"location":"replication/#deploying-our-application-pods","text":"","title":"Deploying our application: pods"},{"location":"services/","text":"Deploying our application: pods","title":"Services"},{"location":"services/#deploying-our-application-pods","text":"","title":"Deploying our application: pods"},{"location":"containers/building-docker-apps/","text":"Building our first Docker application As we've alredy mentioned during the chapter's intro, we are going to use the contacts application as the base of all our examples. The contacts application is a tiny Go service that connects to a MySQL database and allows you to handle your contacts list with a basic set of operations like adding a new contact, or deleting an existing one through an HTTP API. NOTE : it's not required to be familiar with the Go programming language or with MySQL at all, so don't worry if you have never worked with any of these technologies, we will be focusing on the deployment/running aspects. Creating our first Dockerfile The first thing we need to do is creating a Dockerfile file, where we can define the set of steps that Docker needs to perform in order to build the final image. Create an empty file and name it Dockerfile in the contacts folder: and type something like this: FROM golang:1.17 As we've said before, Docker uses a layered approach so we're indicating here that we're building our image from the image golang:1.17 . This image has already installed all the different things that we need to build our application Now that we have the base image, let's copy the source code of our application into our image: FROM golang:1.17 WORKDIR /code COPY . . In the previous snippet we're defining the working dir (all the future commands we execute during the build process will be run in this directory) and copy all our files to the destination images. At this point, we are ready to build our application within the container, so let's run our build process: FROM golang:1.17 WORKDIR /code COPY . . RUN make build The RUN command allow us to define the commands that will be executed during the image build process. In our example, our build process consists in running the build target defined in our Makefile . We're almost done: we've copied everything over from our machine to the image container and built the final executable. Now, we need to define the command that will be run when this image is executed when a container is run: FROM golang:1.17 WORKDIR /code COPY . . RUN make build CMD [ \"bin/contacts.linux\" ] Creating the image In the previous section we defined all the steps required to build the image of our application, but we haven't created the image itself; let's go ahead and create the image. From your contacts application folder, run: docker build -t contacts-application . Once the previous command has successfully finished, the newly created image will live in our local Docker registry. As I am sure you've already guessed, this is not extremely useful because you cannot share this image with anyone else. The real power of Docker is allowing to share the images across the whole Docker community. If we go to a terminal and look for the images registered in our local registry we should see something similar to: > docker images REPOSITORY TAG IMAGE ID CREATED SIZE contacts-application latest 8aa2ee3076db 35 seconds ago 958MB Of course, this is just a quick intro to the image creation process and there're tons of differents things that can be done: multi-stage builds, publishing to a remote registry, optimizing the size of our images, ... sadly, we don't have time to go through all the details, but I encorauge you to take a look to the Docker Getting Started guide where you will find a really nice overview about how to get comfortable with Docker. Let's move to the next section and see how we can run our image using the Docker runtime.","title":"Building applications with Docker"},{"location":"containers/building-docker-apps/#building-our-first-docker-application","text":"As we've alredy mentioned during the chapter's intro, we are going to use the contacts application as the base of all our examples. The contacts application is a tiny Go service that connects to a MySQL database and allows you to handle your contacts list with a basic set of operations like adding a new contact, or deleting an existing one through an HTTP API. NOTE : it's not required to be familiar with the Go programming language or with MySQL at all, so don't worry if you have never worked with any of these technologies, we will be focusing on the deployment/running aspects.","title":"Building our first Docker application"},{"location":"containers/building-docker-apps/#creating-our-first-dockerfile","text":"The first thing we need to do is creating a Dockerfile file, where we can define the set of steps that Docker needs to perform in order to build the final image. Create an empty file and name it Dockerfile in the contacts folder: and type something like this: FROM golang:1.17 As we've said before, Docker uses a layered approach so we're indicating here that we're building our image from the image golang:1.17 . This image has already installed all the different things that we need to build our application Now that we have the base image, let's copy the source code of our application into our image: FROM golang:1.17 WORKDIR /code COPY . . In the previous snippet we're defining the working dir (all the future commands we execute during the build process will be run in this directory) and copy all our files to the destination images. At this point, we are ready to build our application within the container, so let's run our build process: FROM golang:1.17 WORKDIR /code COPY . . RUN make build The RUN command allow us to define the commands that will be executed during the image build process. In our example, our build process consists in running the build target defined in our Makefile . We're almost done: we've copied everything over from our machine to the image container and built the final executable. Now, we need to define the command that will be run when this image is executed when a container is run: FROM golang:1.17 WORKDIR /code COPY . . RUN make build CMD [ \"bin/contacts.linux\" ]","title":"Creating our first Dockerfile"},{"location":"containers/building-docker-apps/#creating-the-image","text":"In the previous section we defined all the steps required to build the image of our application, but we haven't created the image itself; let's go ahead and create the image. From your contacts application folder, run: docker build -t contacts-application . Once the previous command has successfully finished, the newly created image will live in our local Docker registry. As I am sure you've already guessed, this is not extremely useful because you cannot share this image with anyone else. The real power of Docker is allowing to share the images across the whole Docker community. If we go to a terminal and look for the images registered in our local registry we should see something similar to: > docker images REPOSITORY TAG IMAGE ID CREATED SIZE contacts-application latest 8aa2ee3076db 35 seconds ago 958MB Of course, this is just a quick intro to the image creation process and there're tons of differents things that can be done: multi-stage builds, publishing to a remote registry, optimizing the size of our images, ... sadly, we don't have time to go through all the details, but I encorauge you to take a look to the Docker Getting Started guide where you will find a really nice overview about how to get comfortable with Docker. Let's move to the next section and see how we can run our image using the Docker runtime.","title":"Creating the image"},{"location":"containers/intro/","text":"Containers? What is this? Our applications are usually comprised of a language runtime, a bunch of libraries, and, of course, our source code. If we want to make sure that our program is going to properly execute we need to make sure that all our dependencies are installed in the target machine. We've been advocating for an immutable infrastructure during the course, and this is where the container technologies come into place. When we're building our applications is helpful to package them in a way that easily allows us to share them with other people, and this technologies like Docker allow us to easily achieve that goal. During the rest of the book, we will be using the Contacts application available here to apply everything that we are planning to learn. In this particular chapter, we will use the aforementioned application to build and run Docker images. In general, container techs bundle a program and all its required dependencies into a single image under a root filesystem. The Docker image format is the most popular one and it has been standardized by the Open Container Initiative as the OCI image format. What's a container image? Container images are binary packages that bundle everything that's required to run a program inside a container. As we've already mentioned before, the Docker image format is the most widespread one (we dont have enough time to go deeper into how the container images work under the hood, but if you're interested, this has some details. From a really high level perspective, the Docker image is composed by a set of filesystem layers where each of them adds/removes/mododifies layers from the preceeding layer (this is also know as an overly filesystem). What are we going to learn? In this chapter we will get familiar with Docker and will learn how to build an image for our application and how to run the aforementioned image.","title":"Introduction"},{"location":"containers/intro/#containers-what-is-this","text":"Our applications are usually comprised of a language runtime, a bunch of libraries, and, of course, our source code. If we want to make sure that our program is going to properly execute we need to make sure that all our dependencies are installed in the target machine. We've been advocating for an immutable infrastructure during the course, and this is where the container technologies come into place. When we're building our applications is helpful to package them in a way that easily allows us to share them with other people, and this technologies like Docker allow us to easily achieve that goal. During the rest of the book, we will be using the Contacts application available here to apply everything that we are planning to learn. In this particular chapter, we will use the aforementioned application to build and run Docker images. In general, container techs bundle a program and all its required dependencies into a single image under a root filesystem. The Docker image format is the most popular one and it has been standardized by the Open Container Initiative as the OCI image format.","title":"Containers? What is this?"},{"location":"containers/intro/#whats-a-container-image","text":"Container images are binary packages that bundle everything that's required to run a program inside a container. As we've already mentioned before, the Docker image format is the most widespread one (we dont have enough time to go deeper into how the container images work under the hood, but if you're interested, this has some details. From a really high level perspective, the Docker image is composed by a set of filesystem layers where each of them adds/removes/mododifies layers from the preceeding layer (this is also know as an overly filesystem).","title":"What's a container image?"},{"location":"containers/intro/#what-are-we-going-to-learn","text":"In this chapter we will get familiar with Docker and will learn how to build an image for our application and how to run the aforementioned image.","title":"What are we going to learn?"},{"location":"containers/running-docker-apps/","text":"Running a container Let's use the Docker command-line tool to get familar with how to run our containers. As we will learn later during the course, Kubernetes uses a daemon on each node ( kubelet ) to launch the containers, but using the command line will make our first steps simpler. Running our contacts application If you remember from our previous section, we created a Docker image that bundled our contacts application. Let's try to get a running version of our application. Running a MySQL dependency Our contacts application uses MySQL as the datastore to store the contacts, so we will need a running database if we want our contacts application to work as expected. If you recall from our previous section, we mentioned that one of the main benefits of Docker was the ability to share the images we build across all the Docker users, so, let's benefit from this ... Go to your console, to the contacts application folder, and type something similar to: > docker run \\ --rm \\ --detach \\ --name=mysql-contacts-database \\ --env=\"MYSQL_ROOT_PASSWORD=test\" \\ --publish 3306:3306 \\ --volume=$(pwd)/database:/docker-entrypoint-initdb.d \\ mysql:8 mysqld --default-authentication-plugin=mysql_native_password The previous command will run a MySQL container, and will execute all the SQL scripts present in the database folder (note we're mapping our local folder database to the /docker-entrypoint-initdb.d in the running container). Now, we can list the running containers: \u279c contacts docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e7cad8beda4f mysql:8 \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 0.0.0.0:3306->3306/tcp, 33060/tcp mysql-contacts-database And we can even run a terminal into the newly created container and check that the database is running and our database has been created: \u279c contacts docker exec -it e7cad8beda4f /bin/bash root@e7cad8beda4f:/# mysql -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 9 Server version: 8.0.27 MySQL Community Server - GPL Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> use contacts; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +--------------------+ | Tables_in_contacts | +--------------------+ | contacts | +--------------------+ 1 row in set (0.00 sec) mysql> Running the contacts application itself Now that we have our database up and running, we are going to run spawn a container with our Contacts application and get it connected to the previous database. If you recall from our previous section, we created an image with everything required to run our application. Go to your terminal and list your available images: REPOSITORY TAG IMAGE ID CREATED SIZE contacts-application latest 5493a72b86ed 4 hours ago 958MB mysql 8 b05128b000dd 2 weeks ago 516MB Let's use the contacts-application image to spawn a new container: docker run --rm \\ --name=contacts-service \\ --env=\"MYSQL_CONTACTS_DSN=root:test@tcp(172.17.0.2)/contacts\" \\ --publish 8080:8080 contacts-application In the previous command we're telling Docker to use the contacts-application image and pass it the MYSQL_CONTACTS_DSN environment variable. Additonally, we're making the 8080 port available so we can reach the service from our computer. If everything goes as expected, we should be able to reach our application at localhost:8080 . Let's go back to the terminal and give it a try: List the contacts: \u279c ~ curl localhost:8080/contacts | jq . []% Insert a new contact: \u279c ~ curl -XPOST localhost:8080/contact/new -d ' { \"Username\": \"migue\", \"Email\": \"migue@migue.io\", \"Phone\": \"123456789\" }' | jq . {\"Username\":\"migue\",\"Email\":\"migue@migue.io\",\"Phone\":\"123456789\"}% List the contacts again: \u279c ~ curl localhost:8080/contacts | jq . [ { \"Username\": \"migue\", \"Email\": \"migue@migue.io\", \"Phone\": \"123456789\" } ] I guess you've already wondered: how the application connects to the database or where does the 172.17.0.2 come from? If you remember from our previous lessons, we've treat our backing services (MySQL in our case) as attached resources, and this is what we're doing: cfg := conf . Configuration { ServerAddress : \":8080\" , MySQLDSN : os . Getenv ( \"MYSQL_CONTACTS_DSN\" ), } application , err := app . New ( & cfg ) You can see how we're reading the MySQL connection environment from the OS environment, allowing us to inject (using --env ) the corresponding information depending on where we're running our application. And, what about the 172.17.0.2 ? When we created the MySQL container the Docker engine created a network namespace and a bridge network which is reponsible for forwarding the traffic between network segments. This allows different containers connected to the same bridge network to communicate, and, at the same time, provide isolatoin from containers which are not connected to the aforementioned network. We are not going to cover all the details about how Docker networking works, but, for those interested, the official docs provide a really nice introduction. You can go to your terminal and run the docker inspect command and look for the bridge IP: docker inspect mysql-contacts-database | jq '.[].NetworkSettings.Networks.bridge.IPAddress' \"172.17.0.2\" Of course, the docker inspect command provides way much more information: docker inspect mysql-contacts-database [ { \"Id\": \"08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f\", \"Created\": \"2021-12-03T10:44:43.546330197Z\", \"Path\": \"docker-entrypoint.sh\", \"Args\": [ \"mysqld\", \"--default-authentication-plugin=mysql_native_password\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 2117, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2021-12-03T10:44:43.853309044Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" }, \"Image\": \"sha256:b05128b000ddbafb0a0d2713086c6a1cc23280dee3529d37f03c98c97c8cf1ed\", \"ResolvConfPath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/hostname\", \"HostsPath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/hosts\", \"LogPath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f-json.log\", \"Name\": \"/mysql-contacts-database\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": { \"Binds\": [ \"/Users/migue/development/sourcecode/migue/kubernetes-intro/apps/contacts/database:/docker-entrypoint-initdb.d\" ], \"ContainerIDFile\": \"\", \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} }, \"NetworkMode\": \"default\", \"PortBindings\": { \"3306/tcp\": [ { \"HostIp\": \"\", \"HostPort\": \"3306\" } ] }, \"RestartPolicy\": { \"Name\": \"no\", \"MaximumRetryCount\": 0 }, \"AutoRemove\": true, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"CgroupnsMode\": \"host\", \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] }, \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265-init/diff:/var/lib/docker/overlay2/2f9e6cb0fea4bad4f1fd0fee90e2a40bb53ce0d8b045f37b4f20686d941714bb/diff:/var/lib/docker/overlay2/b02899c859ada8bcbcd5748e03d837c39c901b5bfd739cb392f24c76ce6a5609/diff:/var/lib/docker/overlay2/38822ac0f87a0100177d35c5eb4c34b2a9a3e157c34d4c103d74cb6d8cca8c66/diff:/var/lib/docker/overlay2/43d32d57c87dfe795d4b65353d40f7fc8fb5fe446fb014284d5c1d3553b38913/diff:/var/lib/docker/overlay2/59579511fc9d13407ddbe7b22bf55349edf0382677bf702d6d80286eef838239/diff:/var/lib/docker/overlay2/2cd9935313fe6ce6af41d4eafc23337aea69247908849cbd8a25864020cabf6b/diff:/var/lib/docker/overlay2/16dc98f0f26241dce760f8f263bbc0e3e00348d237ccf01535c57ece7df7122a/diff:/var/lib/docker/overlay2/df849902859bbf36fa31cd5374b8481697beeb1f5529709098d2c7de113b021d/diff:/var/lib/docker/overlay2/43a477551fa630b377408a4c3efd29e70fe49c224815b13729cd41daee575cdf/diff:/var/lib/docker/overlay2/79d7bdac1944f40f67369eb568b25b5d9f3ba24afc82b7d7e72a25ac0a17ebcb/diff:/var/lib/docker/overlay2/cd638dbf1a11b548e3a667572d95511139ad0d31f2f0859a28aa8caef898e67a/diff:/var/lib/docker/overlay2/b1651b3a497595b3630c90bb2da5c49a53805d551553c54d95dc2f2867e82515/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265/work\" }, \"Name\": \"overlay2\" }, \"Mounts\": [ { \"Type\": \"bind\", \"Source\": \"/Users/migue/development/sourcecode/migue/kubernetes-intro/apps/contacts/database\", \"Destination\": \"/docker-entrypoint-initdb.d\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" }, { \"Type\": \"volume\", \"Name\": \"03a39f42b334399359c29a2cee6445f5f79a8a4b61b819a15598d684d9d6eb14\", \"Source\": \"/var/lib/docker/volumes/03a39f42b334399359c29a2cee6445f5f79a8a4b61b819a15598d684d9d6eb14/_data\", \"Destination\": \"/var/lib/mysql\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" } ], \"Config\": { \"Hostname\": \"08ca14e9c794\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"ExposedPorts\": { \"3306/tcp\": {}, \"33060/tcp\": {} }, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"MYSQL_ROOT_PASSWORD=test\", \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"GOSU_VERSION=1.12\", \"MYSQL_MAJOR=8.0\", \"MYSQL_VERSION=8.0.27-1debian10\" ], \"Cmd\": [ \"mysqld\", \"--default-authentication-plugin=mysql_native_password\" ], \"Image\": \"mysql:8\", \"Volumes\": { \"/var/lib/mysql\": {} }, \"WorkingDir\": \"\", \"Entrypoint\": [ \"docker-entrypoint.sh\" ], \"OnBuild\": null, \"Labels\": {} }, \"NetworkSettings\": { \"Bridge\": \"\", \"SandboxID\": \"77a394c1bf7f5bdbdde66960cd0e9cffa808d77f1bf2b72110a593856fd77c5d\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": { \"3306/tcp\": [ { \"HostIp\": \"0.0.0.0\", \"HostPort\": \"3306\" } ], \"33060/tcp\": null }, \"SandboxKey\": \"/var/run/docker/netns/77a394c1bf7f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"068727a2228a07db6509da3e01360c521f55130bd163254f40cd45742ddb222a\", \"Gateway\": \"172.17.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:11:00:02\", \"Networks\": { \"bridge\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"6781bedb6144d16b3a0e5c862a2724b68d12b8761de3ee73ed15e18c241be774\", \"EndpointID\": \"068727a2228a07db6509da3e01360c521f55130bd163254f40cd45742ddb222a\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:02\", \"DriverOpts\": null } } } } ] Closing thoughts During this chapter we've been introduced to the container world and we've gone through the basics: Learn what containers and images are Learn how to build an image and package our own applicatoin Learn how to run an existing image (MySQL) Learn how to run our previously built image Learn how to connect the two previous containers so they can talk to each other With this (little) knowledge we are going to move forward and learn how we could deploy our application in a container orchestrator (Kubernetes) and learn the fundamentals about it.","title":"Runnig applications with Docker"},{"location":"containers/running-docker-apps/#running-a-container","text":"Let's use the Docker command-line tool to get familar with how to run our containers. As we will learn later during the course, Kubernetes uses a daemon on each node ( kubelet ) to launch the containers, but using the command line will make our first steps simpler.","title":"Running a container"},{"location":"containers/running-docker-apps/#running-our-contacts-application","text":"If you remember from our previous section, we created a Docker image that bundled our contacts application. Let's try to get a running version of our application.","title":"Running our contacts application"},{"location":"containers/running-docker-apps/#running-a-mysql-dependency","text":"Our contacts application uses MySQL as the datastore to store the contacts, so we will need a running database if we want our contacts application to work as expected. If you recall from our previous section, we mentioned that one of the main benefits of Docker was the ability to share the images we build across all the Docker users, so, let's benefit from this ... Go to your console, to the contacts application folder, and type something similar to: > docker run \\ --rm \\ --detach \\ --name=mysql-contacts-database \\ --env=\"MYSQL_ROOT_PASSWORD=test\" \\ --publish 3306:3306 \\ --volume=$(pwd)/database:/docker-entrypoint-initdb.d \\ mysql:8 mysqld --default-authentication-plugin=mysql_native_password The previous command will run a MySQL container, and will execute all the SQL scripts present in the database folder (note we're mapping our local folder database to the /docker-entrypoint-initdb.d in the running container). Now, we can list the running containers: \u279c contacts docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e7cad8beda4f mysql:8 \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 0.0.0.0:3306->3306/tcp, 33060/tcp mysql-contacts-database And we can even run a terminal into the newly created container and check that the database is running and our database has been created: \u279c contacts docker exec -it e7cad8beda4f /bin/bash root@e7cad8beda4f:/# mysql -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 9 Server version: 8.0.27 MySQL Community Server - GPL Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> use contacts; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +--------------------+ | Tables_in_contacts | +--------------------+ | contacts | +--------------------+ 1 row in set (0.00 sec) mysql>","title":"Running a MySQL dependency"},{"location":"containers/running-docker-apps/#running-the-contacts-application-itself","text":"Now that we have our database up and running, we are going to run spawn a container with our Contacts application and get it connected to the previous database. If you recall from our previous section, we created an image with everything required to run our application. Go to your terminal and list your available images: REPOSITORY TAG IMAGE ID CREATED SIZE contacts-application latest 5493a72b86ed 4 hours ago 958MB mysql 8 b05128b000dd 2 weeks ago 516MB Let's use the contacts-application image to spawn a new container: docker run --rm \\ --name=contacts-service \\ --env=\"MYSQL_CONTACTS_DSN=root:test@tcp(172.17.0.2)/contacts\" \\ --publish 8080:8080 contacts-application In the previous command we're telling Docker to use the contacts-application image and pass it the MYSQL_CONTACTS_DSN environment variable. Additonally, we're making the 8080 port available so we can reach the service from our computer. If everything goes as expected, we should be able to reach our application at localhost:8080 . Let's go back to the terminal and give it a try: List the contacts: \u279c ~ curl localhost:8080/contacts | jq . []% Insert a new contact: \u279c ~ curl -XPOST localhost:8080/contact/new -d ' { \"Username\": \"migue\", \"Email\": \"migue@migue.io\", \"Phone\": \"123456789\" }' | jq . {\"Username\":\"migue\",\"Email\":\"migue@migue.io\",\"Phone\":\"123456789\"}% List the contacts again: \u279c ~ curl localhost:8080/contacts | jq . [ { \"Username\": \"migue\", \"Email\": \"migue@migue.io\", \"Phone\": \"123456789\" } ] I guess you've already wondered: how the application connects to the database or where does the 172.17.0.2 come from? If you remember from our previous lessons, we've treat our backing services (MySQL in our case) as attached resources, and this is what we're doing: cfg := conf . Configuration { ServerAddress : \":8080\" , MySQLDSN : os . Getenv ( \"MYSQL_CONTACTS_DSN\" ), } application , err := app . New ( & cfg ) You can see how we're reading the MySQL connection environment from the OS environment, allowing us to inject (using --env ) the corresponding information depending on where we're running our application. And, what about the 172.17.0.2 ? When we created the MySQL container the Docker engine created a network namespace and a bridge network which is reponsible for forwarding the traffic between network segments. This allows different containers connected to the same bridge network to communicate, and, at the same time, provide isolatoin from containers which are not connected to the aforementioned network. We are not going to cover all the details about how Docker networking works, but, for those interested, the official docs provide a really nice introduction. You can go to your terminal and run the docker inspect command and look for the bridge IP: docker inspect mysql-contacts-database | jq '.[].NetworkSettings.Networks.bridge.IPAddress' \"172.17.0.2\" Of course, the docker inspect command provides way much more information: docker inspect mysql-contacts-database [ { \"Id\": \"08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f\", \"Created\": \"2021-12-03T10:44:43.546330197Z\", \"Path\": \"docker-entrypoint.sh\", \"Args\": [ \"mysqld\", \"--default-authentication-plugin=mysql_native_password\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 2117, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2021-12-03T10:44:43.853309044Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" }, \"Image\": \"sha256:b05128b000ddbafb0a0d2713086c6a1cc23280dee3529d37f03c98c97c8cf1ed\", \"ResolvConfPath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/hostname\", \"HostsPath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/hosts\", \"LogPath\": \"/var/lib/docker/containers/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f/08ca14e9c794481c3a164e192720d09e426a6b7e4fe535561007ae9c285cba0f-json.log\", \"Name\": \"/mysql-contacts-database\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": { \"Binds\": [ \"/Users/migue/development/sourcecode/migue/kubernetes-intro/apps/contacts/database:/docker-entrypoint-initdb.d\" ], \"ContainerIDFile\": \"\", \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} }, \"NetworkMode\": \"default\", \"PortBindings\": { \"3306/tcp\": [ { \"HostIp\": \"\", \"HostPort\": \"3306\" } ] }, \"RestartPolicy\": { \"Name\": \"no\", \"MaximumRetryCount\": 0 }, \"AutoRemove\": true, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"CgroupnsMode\": \"host\", \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] }, \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265-init/diff:/var/lib/docker/overlay2/2f9e6cb0fea4bad4f1fd0fee90e2a40bb53ce0d8b045f37b4f20686d941714bb/diff:/var/lib/docker/overlay2/b02899c859ada8bcbcd5748e03d837c39c901b5bfd739cb392f24c76ce6a5609/diff:/var/lib/docker/overlay2/38822ac0f87a0100177d35c5eb4c34b2a9a3e157c34d4c103d74cb6d8cca8c66/diff:/var/lib/docker/overlay2/43d32d57c87dfe795d4b65353d40f7fc8fb5fe446fb014284d5c1d3553b38913/diff:/var/lib/docker/overlay2/59579511fc9d13407ddbe7b22bf55349edf0382677bf702d6d80286eef838239/diff:/var/lib/docker/overlay2/2cd9935313fe6ce6af41d4eafc23337aea69247908849cbd8a25864020cabf6b/diff:/var/lib/docker/overlay2/16dc98f0f26241dce760f8f263bbc0e3e00348d237ccf01535c57ece7df7122a/diff:/var/lib/docker/overlay2/df849902859bbf36fa31cd5374b8481697beeb1f5529709098d2c7de113b021d/diff:/var/lib/docker/overlay2/43a477551fa630b377408a4c3efd29e70fe49c224815b13729cd41daee575cdf/diff:/var/lib/docker/overlay2/79d7bdac1944f40f67369eb568b25b5d9f3ba24afc82b7d7e72a25ac0a17ebcb/diff:/var/lib/docker/overlay2/cd638dbf1a11b548e3a667572d95511139ad0d31f2f0859a28aa8caef898e67a/diff:/var/lib/docker/overlay2/b1651b3a497595b3630c90bb2da5c49a53805d551553c54d95dc2f2867e82515/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/567e028d45ff25519ce3a4dc734ceacd52e08f9bb608782a5d2a3ece1b06b265/work\" }, \"Name\": \"overlay2\" }, \"Mounts\": [ { \"Type\": \"bind\", \"Source\": \"/Users/migue/development/sourcecode/migue/kubernetes-intro/apps/contacts/database\", \"Destination\": \"/docker-entrypoint-initdb.d\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" }, { \"Type\": \"volume\", \"Name\": \"03a39f42b334399359c29a2cee6445f5f79a8a4b61b819a15598d684d9d6eb14\", \"Source\": \"/var/lib/docker/volumes/03a39f42b334399359c29a2cee6445f5f79a8a4b61b819a15598d684d9d6eb14/_data\", \"Destination\": \"/var/lib/mysql\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" } ], \"Config\": { \"Hostname\": \"08ca14e9c794\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"ExposedPorts\": { \"3306/tcp\": {}, \"33060/tcp\": {} }, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"MYSQL_ROOT_PASSWORD=test\", \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"GOSU_VERSION=1.12\", \"MYSQL_MAJOR=8.0\", \"MYSQL_VERSION=8.0.27-1debian10\" ], \"Cmd\": [ \"mysqld\", \"--default-authentication-plugin=mysql_native_password\" ], \"Image\": \"mysql:8\", \"Volumes\": { \"/var/lib/mysql\": {} }, \"WorkingDir\": \"\", \"Entrypoint\": [ \"docker-entrypoint.sh\" ], \"OnBuild\": null, \"Labels\": {} }, \"NetworkSettings\": { \"Bridge\": \"\", \"SandboxID\": \"77a394c1bf7f5bdbdde66960cd0e9cffa808d77f1bf2b72110a593856fd77c5d\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": { \"3306/tcp\": [ { \"HostIp\": \"0.0.0.0\", \"HostPort\": \"3306\" } ], \"33060/tcp\": null }, \"SandboxKey\": \"/var/run/docker/netns/77a394c1bf7f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"068727a2228a07db6509da3e01360c521f55130bd163254f40cd45742ddb222a\", \"Gateway\": \"172.17.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:11:00:02\", \"Networks\": { \"bridge\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"6781bedb6144d16b3a0e5c862a2724b68d12b8761de3ee73ed15e18c241be774\", \"EndpointID\": \"068727a2228a07db6509da3e01360c521f55130bd163254f40cd45742ddb222a\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:02\", \"DriverOpts\": null } } } } ]","title":"Running the contacts application itself"},{"location":"containers/running-docker-apps/#closing-thoughts","text":"During this chapter we've been introduced to the container world and we've gone through the basics: Learn what containers and images are Learn how to build an image and package our own applicatoin Learn how to run an existing image (MySQL) Learn how to run our previously built image Learn how to connect the two previous containers so they can talk to each other With this (little) knowledge we are going to move forward and learn how we could deploy our application in a container orchestrator (Kubernetes) and learn the fundamentals about it.","title":"Closing thoughts"}]}